{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Cleaning of news data\n",
    "#Extra cleaning on the news data? \n",
    "#Clean out bad articles\n",
    "\n",
    "#TODO: Long.Short Name Conversion\n",
    "#Long name to many short names\n",
    "\n",
    "#TODO: Article analysis on number of false positives\n",
    "#Look at how articles mention each c\n",
    "# ompany \n",
    "#Eyeball false positive to true positives \n",
    "#False positives -> Create manual labels \n",
    "\n",
    "#TODO: NEgative Examples\n",
    "#Negative Examples\n",
    "#Negative example from articles that mentions company A from KB and any entity recognised by spaCy\n",
    "#Use spacy tags to create negative examples?\n",
    "\n",
    "#TODO: More Spacy Integration and Language Model Integration\n",
    "#Use cusutom tokenizier instead of regex to match entities\n",
    "#E.g. Spacy POS Tags to be used as features\n",
    "#Figure out where inthe process a large language model would fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries \n",
    "Changing Path to import nlp functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import gzip\n",
    "import itertools\n",
    "\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "\n",
    "try:\n",
    "    # python package (nlp) location - two levels up from this file\n",
    "    src_path = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "    # add package to sys.path if it's not already there\n",
    "    if src_path not in sys.path:\n",
    "        sys.path.extend([src_path])\n",
    "except NameError:\n",
    "    print('issue with adding to path, probably due to __file__ not being defined')\n",
    "    src_path = None\n",
    "    \n",
    "from nlp.utils import get_database\n",
    "from nlp import get_configs_path, get_data_path\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_articles_with_name(articles, name):\n",
    "    return [v for k, v in articles.items() if re.search(name, v[\"maintext\"])]\n",
    "\n",
    "\n",
    "def remove_suffix(name, suffixes):\n",
    "    for s in suffixes:\n",
    "        # regex: space, word, space then any character to end\n",
    "        # or\n",
    "        name = re.sub(f\" {s} .*$| {s}$\", \"\", name)\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_start_end(a, b, aname=\"a\", bname=\"b\"):\n",
    "\n",
    "    assert len(np.intersect1d(a, b)) == 0, f\"some elements found in both 'a' and 'b'\"\n",
    "\n",
    "    c = np.sort(np.concatenate([a, b]))\n",
    "    is_a = np.in1d(c, a)\n",
    "\n",
    "    res = []\n",
    "    names = []\n",
    "    for i in range(len(c)-1):\n",
    "        if is_a[i] != is_a[i+1]:\n",
    "            res.append((c[i], c[i+1]))\n",
    "            if is_a[i]:\n",
    "                names.append((aname, bname))\n",
    "            else:\n",
    "                names.append((bname, aname))\n",
    "\n",
    "    return res, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get credentials\n",
    "with open(get_configs_path(\"mongo.json\"), \"r+\") as f:\n",
    "    mdb_cred = json.load(f)\n",
    "\n",
    "# get mongodb client - for connections\n",
    "client = get_database(username=mdb_cred[\"username\"],\n",
    "                        password=mdb_cred[\"password\"],\n",
    "                        clustername=mdb_cred[\"cluster_name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read in: 81275 articles\n"
     ]
    }
   ],
   "source": [
    "#Parameter:\n",
    "read_local_articles = False\n",
    "\n",
    "# ----\n",
    "# read the articles in\n",
    "# ----\n",
    "\n",
    "if read_local_articles:\n",
    "    # read proof of concept data\n",
    "    with open(get_data_path(\"articles.json\"), \"r\") as f:\n",
    "        article_list = json.load(f)\n",
    "\n",
    "    #\n",
    "    articles = {re.sub(\"\\.json$\", \"\", f[\"json_file\"]): f\n",
    "                for f in article_list if \"names_in_text\" in f}\n",
    "\n",
    "else:\n",
    "    # instead, read (a larger) set of articles from mongo\n",
    "    # - this can be quite large\n",
    "    # because taking 'names_in_text' this might not be everything\n",
    "    articles = {re.sub(\"\\.json$\", \"\", f[\"json_file\"]): f\n",
    "                for f in client[\"news_articles\"][\"articles\"].find({'names_in_text': {\"$exists\": True}})}\n",
    "\n",
    "keys = list(articles.keys())\n",
    "\n",
    "print(f\"read in: {len(keys)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database names: ['news_articles', 'refinitiv', 'test', 'admin', 'local']\n"
     ]
    }
   ],
   "source": [
    "#TODO: Connect to DB to load latest mapping \n",
    "print(f\"database names: {client.list_database_names()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('620d7157c02377e1232d8ed7'),\n",
       " 'authors': ['Reuters Editorial', 'Reuters Staff'],\n",
       " 'date_download': '2018-03-20 02:27:54+00:00',\n",
       " 'date_modify': None,\n",
       " 'date_publish': '2018-03-19 11:41:38',\n",
       " 'description': 'Oil giant Saudi Aramco is looking to buy a majority stake in a proposed refinery and petrochemical complex valued at 3 trillion rupees ($46.10 billion) in the western Indian state of Maharashtra, local daily Economic Times reported, citing people familiar with the matter.',\n",
       " 'filename': 'https%3A%2F%2Fwww.reuters.com%2Farticle%2Fus-saudiaramco-india%2Fsaudi-aramco-seeks-majority-stake-in-indian-refinery-economic-times-idUSKBN1GV1B3.json',\n",
       " 'image_url': 'https://s4.reutersmedia.net/resources/r/?m=02&d=20180319&t=2&i=1242461315&w=1200&r=LYNXMPEE2I0T8',\n",
       " 'language': 'en',\n",
       " 'localpath': None,\n",
       " 'maintext': '(Reuters) - Oil giant Saudi Aramco is looking to buy a majority stake in a proposed refinery and petrochemical complex valued at 3 trillion rupees ($46.10 billion) in the western Indian state of Maharashtra, local daily Economic Times reported, citing people familiar with the matter.\\nFILE PHOTO: Logo of Saudi Aramco is seen at the 20th Middle East Oil & Gas Show and Conference (MOES 2017) in Manama, Bahrain, March 7, 2017. REUTERS/Hamad I Mohammed/File Photo\\nThis comes as Saudi Arabia, the world’s biggest oil exporter, is trying to beat out Iraq to become the biggest crude supplier to India, the world’s third-largest oil importer.\\nThe state-owned Saudi oil company is also in talks with Indian state-run refiners for marketing rights over the fuel and petrochemicals produced at the complex along with an assurance that the refinery would mostly use Saudi oil, the newspaper added.\\nIndian Oil Corp Ltd owns 50 percent of the complex, while Bharat Petroleum Corp Ltd and Hindustan Petroleum Corp Ltd have the rest.\\nSaudi Aramco was not immediately available for comment.\\nThe refinery is expected to have a more than 300,000 barrels per day throughput capacity and to be commissioned by 2022, feeding a country that is among the biggest drivers of energy demand in the world.\\nSaudi Arabia is planning to list up to 5 percent of Saudi Aramco in an initial public offering that could value the company at up to $2 trillion, making it the world’s biggest oil company by market capitalization.\\n($1 = 65.0775 Indian rupees)',\n",
       " 'source_domain': 'www.reuters.com',\n",
       " 'title': 'Saudi Aramco seeks majority stake in Indian refinery: Economic Times',\n",
       " 'title_page': None,\n",
       " 'title_rss': None,\n",
       " 'url': 'https://www.reuters.com/article/us-saudiaramco-india/saudi-aramco-seeks-majority-stake-in-indian-refinery-economic-times-idUSKBN1GV1B3',\n",
       " 'json_file': '99590514bc9e3606df46982f0e37d96666fb89c49ee86267a6ad269d93f9e2d1.json',\n",
       " 'names_in_text': ['Hindustan Petroleum Corp Ltd']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pulling one Article from the Database \n",
    "articles[list(articles.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check how many articles have the 'names_in_text attribute\n",
    "# print(\"{} out of {} have the 'names in text' attribute\".format(len(comp_lists),len(articles.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 120852 company mentions across all 81275 articles that have the names_in_text attribute\n"
     ]
    }
   ],
   "source": [
    "#Create a list of all the companies named in each article (This is a list of lists -> One list per article)\n",
    "comp_lists = [articles[key]['names_in_text'] for key in articles.keys() if 'names_in_text' in articles[key].keys()]\n",
    "#Crate one list that includes all companies in one list\n",
    "comp_list = [company for comp_list in comp_lists for company in comp_list]\n",
    "#Check how many company mentions we have in total\n",
    "print(\"There are {} company mentions across all {} articles that have the names_in_text attribute\".format(len(comp_list),len(comp_lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of Articles\n",
       "1                58515\n",
       "2                13464\n",
       "3                 5195\n",
       "4                 2380\n",
       "5                  924\n",
       "6                  407\n",
       "7                  198\n",
       "8                   94\n",
       "9                   39\n",
       "10                  24\n",
       "11                  12\n",
       "12                  11\n",
       "14                   4\n",
       "20                   3\n",
       "39                   1\n",
       "45                   1\n",
       "18                   1\n",
       "16                   1\n",
       "15                   1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check how many companies are mentioned by each article\n",
    "num_comp_per_article = [len(comp_list) for comp_list in comp_lists]\n",
    "num_comp_per_article_dict = Counter(num_comp_per_article)\n",
    "num_comp_per_article_df = pd.DataFrame(num_comp_per_article_dict.values(),index=num_comp_per_article_dict.keys(),columns=[\"Number of Articles\"]).sort_values(\"Number of Articles\",ascending=False)\n",
    "num_comp_per_article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out which article is the one with 15 (!) company mentions\n",
    "#Commented out because its a large print\n",
    "#print([articles[key] for key in articles.keys() if 'names_in_text' in articles[key].keys() and len(articles[key]['names_in_text'])>14][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check which company is mentioned most frequently -> Classic Power Law distribution\n",
    "comp_dict = Counter(comp_list)\n",
    "comp_df = pd.DataFrame(comp_dict.values(), columns= [\"Occurances\"], index = comp_dict.keys()).sort_values('Occurances', ascending=False)\n",
    "comp_df.plot()\n",
    "comp_df.head(50)\n",
    "print(comp_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occurances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Morgan Stanley</th>\n",
       "      <td>9677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple Inc</th>\n",
       "      <td>8753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boeing Co</th>\n",
       "      <td>4539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amazon.com Inc</th>\n",
       "      <td>4303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Microsoft Corp</th>\n",
       "      <td>3398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General Motors Co</th>\n",
       "      <td>3305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alphabet Inc</th>\n",
       "      <td>2987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ford Motor Co</th>\n",
       "      <td>2836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toyota Motor Corp</th>\n",
       "      <td>2574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tesla Inc</th>\n",
       "      <td>2235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exxon Mobil Corp</th>\n",
       "      <td>1914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intel Corp</th>\n",
       "      <td>1735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chevron Corp</th>\n",
       "      <td>1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT&amp;T Inc</th>\n",
       "      <td>1687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General Electric Co</th>\n",
       "      <td>1633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Samsung Electronics Co Ltd</th>\n",
       "      <td>1615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lockheed Martin Corp</th>\n",
       "      <td>1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volkswagen AG</th>\n",
       "      <td>1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comcast Corp</th>\n",
       "      <td>1322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Netflix Inc</th>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Federal Aviation Administration</th>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toshiba Corp</th>\n",
       "      <td>1161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bombardier Inc</th>\n",
       "      <td>1156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Verizon Communications Inc</th>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qualcomm Inc</th>\n",
       "      <td>1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uber Technologies Inc</th>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alibaba Group Holding Ltd</th>\n",
       "      <td>940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Huawei Technologies Co Ltd</th>\n",
       "      <td>926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vale SA</th>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SoftBank Group Corp</th>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Time Warner Inc</th>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walmart Inc</th>\n",
       "      <td>831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hyundai Motor Co</th>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Panasonic Corp</th>\n",
       "      <td>819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twitter Inc</th>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target Corp</th>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Caterpillar Inc</th>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oracle Corp</th>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Air Canada</th>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharp Corp</th>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta Air Lines Inc</th>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mobil Corp</th>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coca-Cola Co</th>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sprint Corp</th>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phillips 66</th>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American Airlines Group Inc</th>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southwest Airlines Co</th>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FedEx Corp</th>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cisco Systems Inc</th>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T-Mobile US Inc</th>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Occurances\n",
       "Morgan Stanley                         9677\n",
       "Apple Inc                              8753\n",
       "Boeing Co                              4539\n",
       "Amazon.com Inc                         4303\n",
       "Microsoft Corp                         3398\n",
       "General Motors Co                      3305\n",
       "Alphabet Inc                           2987\n",
       "Ford Motor Co                          2836\n",
       "Toyota Motor Corp                      2574\n",
       "Tesla Inc                              2235\n",
       "Exxon Mobil Corp                       1914\n",
       "Intel Corp                             1735\n",
       "Chevron Corp                           1720\n",
       "AT&T Inc                               1687\n",
       "General Electric Co                    1633\n",
       "Samsung Electronics Co Ltd             1615\n",
       "Lockheed Martin Corp                   1573\n",
       "Volkswagen AG                          1462\n",
       "Comcast Corp                           1322\n",
       "Netflix Inc                            1296\n",
       "Federal Aviation Administration        1295\n",
       "Toshiba Corp                           1161\n",
       "Bombardier Inc                         1156\n",
       "Verizon Communications Inc             1063\n",
       "Qualcomm Inc                           1019\n",
       "Uber Technologies Inc                   988\n",
       "Alibaba Group Holding Ltd               940\n",
       "Huawei Technologies Co Ltd              926\n",
       "Vale SA                                 912\n",
       "SoftBank Group Corp                     862\n",
       "Time Warner Inc                         860\n",
       "Walmart Inc                             831\n",
       "Hyundai Motor Co                        829\n",
       "Panasonic Corp                          819\n",
       "Twitter Inc                             792\n",
       "Target Corp                             769\n",
       "Caterpillar Inc                         759\n",
       "Oracle Corp                             757\n",
       "Air Canada                              756\n",
       "Sharp Corp                              735\n",
       "Delta Air Lines Inc                     701\n",
       "Mobil Corp                              668\n",
       "Coca-Cola Co                            657\n",
       "Sprint Corp                             647\n",
       "Phillips 66                             639\n",
       "American Airlines Group Inc             612\n",
       "Southwest Airlines Co                   551\n",
       "FedEx Corp                              548\n",
       "Cisco Systems Inc                       524\n",
       "T-Mobile US Inc                         510"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Supply Chain Data\n",
    "(Look at value_chain_data_exploration.ipnyb for more anlaysis on the graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>Parent Name</th>\n",
       "      <th>Parent Id</th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Confidence Score (%)</th>\n",
       "      <th>Last Update Date</th>\n",
       "      <th>Days Since Last Update</th>\n",
       "      <th>Freshness</th>\n",
       "      <th>Snippet Count</th>\n",
       "      <th>Revenue (USD)</th>\n",
       "      <th>EQ Score</th>\n",
       "      <th>Implied Rating</th>\n",
       "      <th>fetch_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61ffc35c52e17d8ff84f5c3c</td>\n",
       "      <td>DS Smith PLC</td>\n",
       "      <td>4295894074</td>\n",
       "      <td>4295903247</td>\n",
       "      <td>Procter &amp; Gamble Co</td>\n",
       "      <td>Public</td>\n",
       "      <td>Customer</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>Personal Products</td>\n",
       "      <td>0.587784</td>\n",
       "      <td>2015-06-25</td>\n",
       "      <td>2416</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7.611800e+10</td>\n",
       "      <td>89.0</td>\n",
       "      <td>BBB+</td>\n",
       "      <td>2022-02-04 11:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61ffc2e552e17d8ff84f5c37</td>\n",
       "      <td>Seoul Semiconductor Co Ltd</td>\n",
       "      <td>4295882677</td>\n",
       "      <td>4295905573</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Public</td>\n",
       "      <td>Customer</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>Phones &amp; Handheld Devices</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>1465</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.658170e+11</td>\n",
       "      <td>80.0</td>\n",
       "      <td>A</td>\n",
       "      <td>2022-02-05 19:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61ffc35c52e17d8ff84f5c41</td>\n",
       "      <td>Brightstar Corp</td>\n",
       "      <td>4296334059</td>\n",
       "      <td>4295877094</td>\n",
       "      <td>SoftBank Group Corp</td>\n",
       "      <td>Public</td>\n",
       "      <td>Customer</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Wireless Telecommunications Services</td>\n",
       "      <td>0.630819</td>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>2942</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5.309997e+10</td>\n",
       "      <td>7.0</td>\n",
       "      <td>BB-</td>\n",
       "      <td>2022-02-05 20:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61ffc35c52e17d8ff84f5c46</td>\n",
       "      <td>Brightstar Corp</td>\n",
       "      <td>4296334059</td>\n",
       "      <td>4295907020</td>\n",
       "      <td>Logility Inc</td>\n",
       "      <td>Private</td>\n",
       "      <td>Supplier</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>IT Services &amp; Consulting</td>\n",
       "      <td>0.290066</td>\n",
       "      <td>2018-05-15</td>\n",
       "      <td>1362</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.160300e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-05 20:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61ffc35c52e17d8ff84f5c48</td>\n",
       "      <td>Navistar International Corp</td>\n",
       "      <td>4295904608</td>\n",
       "      <td>4295903829</td>\n",
       "      <td>Cummins Inc</td>\n",
       "      <td>Public</td>\n",
       "      <td>Supplier</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>Auto, Truck &amp; Motorcycle Parts</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>2021-05-20</td>\n",
       "      <td>259</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>1.981100e+10</td>\n",
       "      <td>90.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2022-02-03 14:41:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                  Parent Name   Parent Id  \\\n",
       "0  61ffc35c52e17d8ff84f5c3c                 DS Smith PLC  4295894074   \n",
       "1  61ffc2e552e17d8ff84f5c37   Seoul Semiconductor Co Ltd  4295882677   \n",
       "2  61ffc35c52e17d8ff84f5c41              Brightstar Corp  4296334059   \n",
       "3  61ffc35c52e17d8ff84f5c46              Brightstar Corp  4296334059   \n",
       "4  61ffc35c52e17d8ff84f5c48  Navistar International Corp  4295904608   \n",
       "\n",
       "   Identifier         Company Name     Type Relationship  \\\n",
       "0  4295903247  Procter & Gamble Co   Public     Customer   \n",
       "1  4295905573            Apple Inc   Public     Customer   \n",
       "2  4295877094  SoftBank Group Corp   Public     Customer   \n",
       "3  4295907020         Logility Inc  Private     Supplier   \n",
       "4  4295903829          Cummins Inc   Public     Supplier   \n",
       "\n",
       "             Country/Region                              Industry  \\\n",
       "0  United States of America                     Personal Products   \n",
       "1  United States of America             Phones & Handheld Devices   \n",
       "2                     Japan  Wireless Telecommunications Services   \n",
       "3  United States of America              IT Services & Consulting   \n",
       "4  United States of America        Auto, Truck & Motorcycle Parts   \n",
       "\n",
       "   Confidence Score (%) Last Update Date  Days Since Last Update  Freshness  \\\n",
       "0              0.587784       2015-06-25                    2416          1   \n",
       "1              1.000000       2018-02-01                    1465          2   \n",
       "2              0.630819       2014-01-16                    2942          1   \n",
       "3              0.290066       2018-05-15                    1362          2   \n",
       "4              0.999956       2021-05-20                     259          5   \n",
       "\n",
       "   Snippet Count  Revenue (USD)  EQ Score Implied Rating           fetch_time  \n",
       "0              3   7.611800e+10      89.0           BBB+  2022-02-04 11:10:00  \n",
       "1              1   3.658170e+11      80.0              A  2022-02-05 19:47:00  \n",
       "2              4   5.309997e+10       7.0            BB-  2022-02-05 20:25:00  \n",
       "3              2   4.160300e+07       NaN            NaN  2022-02-05 20:25:00  \n",
       "4             23   1.981100e+10      90.0            BBB  2022-02-03 14:41:00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---\n",
    "# read in value chain data\n",
    "# ---\n",
    "\n",
    "vc = pd.DataFrame(list(client[\"refinitiv\"][\"VCHAINS\"].find(filter={})))\n",
    "vc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21458"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of relationships\n",
    "vc.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Num of Parent Companies\n",
    "len(vc['Parent Name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the News Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out articles with incorrect matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after removing erroneous articles, there are now: 22394 articles\n",
      "Counter({'Mobil Corp': 666, 'LifeLock Inc': 5, 'Oracle Corp': 2})\n"
     ]
    }
   ],
   "source": [
    "#This will check for erroneous matches (Mobil Corp is matched for Exxon Mobile in other script, LifeLock and Oracle are also erroneously matched)\n",
    "#Then remove all articles that have only 1 name in it as they will not help us in finding positive or negative samples\n",
    "\n",
    "# get all names\n",
    "all_names = np.unique(np.concatenate([v[\"names_in_text\"]\n",
    "                                        for k, v in articles.items()]))\n",
    "# add a dash in names to have more explicit matching (?)\n",
    "dash_names = {n: re.sub(\" \", \"-\", n) for n in all_names}\n",
    "dash_names\n",
    "# for each article replace the name with the dashed name\n",
    "# - the idea is this will help with tokenizing\n",
    "\n",
    "not_found_list = []\n",
    "\n",
    "names_in_text_dict = {}\n",
    "for k in keys:\n",
    "    a = articles[k]\n",
    "    anames = np.array(a[\"names_in_text\"])\n",
    "    dnames = np.array([dash_names[an] for an in anames])\n",
    "    # replace the longest matching first - to deal with\n",
    "    name_len = [len(an) for an in anames]\n",
    "    name_ord = np.argsort(name_len)[::-1]\n",
    "\n",
    "    maintext = a[\"maintext\"]\n",
    "    name_in_article = []\n",
    "    for an in anames[name_ord]:\n",
    "        if re.search(an, maintext):\n",
    "            # get the new-names  in article\n",
    "            name_in_article.append(an)\n",
    "            maintext = re.sub(an, dash_names[an], maintext)\n",
    "        else:\n",
    "            # print(f\"could not find: {an} in {k}\")\n",
    "            pass\n",
    "            not_found_list.append(an)\n",
    "            #print(\"Apparently could not find {} in the maintext\".format(an))\n",
    "            #print(maintext)\n",
    "            #break\n",
    "    names_in_text_dict[k] = name_in_article\n",
    "    \n",
    "    \n",
    "# keep only those articles with two or more names_in_text\n",
    "articles = {k: articles[k] for k, v in names_in_text_dict.items() if len(v) > 1}\n",
    "\n",
    "print(f\"after removing erroneous articles, there are now: {len(articles)} articles\")\n",
    "\n",
    "#Print how often each company was not found\n",
    "print(Counter(not_found_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are: 4595 that mention supply|supplier|supplies out of 22394 in total\n"
     ]
    }
   ],
   "source": [
    "# ----\n",
    "# find all the articles with supply|supplies|supplier in maintext\n",
    "# ----\n",
    "\n",
    "supply_articles = []\n",
    "for k, v in articles.items():\n",
    "    if re.search(\"supply|supplier|supplies\", v[\"maintext\"]):\n",
    "        supply_articles.append(k)\n",
    "\n",
    "print(f\"there are: {len(supply_articles)} that mention {'supply|supplier|supplies'} out of {len(articles)} in total\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic analysis on FILTERED articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are: 20372 unique titles, 1518 occur more than once\n"
     ]
    }
   ],
   "source": [
    "# ----\n",
    "# title count\n",
    "# ----\n",
    "#Are we using this anywhere at this point?\n",
    "\n",
    "all_titles = {\"titles\": [a['title'] for a in articles.values()]}\n",
    "all_titles = pd.DataFrame(all_titles)\n",
    "all_titles[\"count\"] = 1\n",
    "\n",
    "title_count = pd.pivot_table(all_titles, index=\"titles\", values=\"count\", aggfunc=\"count\")\n",
    "\n",
    "title_count.sort_values(\"count\", ascending=False, inplace=True)\n",
    "\n",
    "print(f\"there are: {len(title_count)} unique titles, {len(title_count.loc[title_count['count']> 1])} occur more than once\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there were 747 companies found in articles\n"
     ]
    }
   ],
   "source": [
    "# ----\n",
    "# for each company in value chain data - count of the number of articles in\n",
    "# ----\n",
    "#We have something similar above -> Can replace / exchange \n",
    "\n",
    "article_count = {}\n",
    "# NOTE: maybe reverse this operation and just cycle through articles\n",
    "# - rather than names and articles ~ O(m*n)\n",
    "for n in all_names:\n",
    "    for k, v in articles.items():\n",
    "        if n in v[\"names_in_text\"]:\n",
    "            if n in article_count:\n",
    "                article_count[n] += 1\n",
    "            else:\n",
    "                article_count[n] = 1\n",
    "\n",
    "print(f\"there were {len(article_count)} companies found in articles\")\n",
    "\n",
    "c_count = pd.DataFrame([(k, v) for k, v in article_count.items()],\n",
    "                        columns=[\"name\", \"in_articles\"])\n",
    "c_count.sort_values(\"in_articles\", ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pairs of companies from Database and store which articles include both-> Key Step for the serach below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/572\n",
      "50/572\n",
      "100/572\n",
      "150/572\n",
      "200/572\n",
      "250/572\n",
      "300/572\n",
      "350/572\n",
      "400/572\n",
      "450/572\n",
      "500/572\n",
      "550/572\n",
      "number of Parent Company's found in articles: 378\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# search for articles containing pair\n",
    "# ---\n",
    "\n",
    "# TODO: refactor this - search for parents first\n",
    "#Why add values as list and not as string entries? This would make it closer like an inverted index\n",
    "\n",
    "# find all the articles that contain parent nane\n",
    "pname = vc[\"Parent Name\"].unique()\n",
    "pdict = {}\n",
    "for i, pn in enumerate(pname):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"{i}/{len(pname)}\")\n",
    "    for k, v in articles.items():\n",
    "        if pn in v[\"names_in_text\"]:\n",
    "            if pn in pdict:\n",
    "                pdict[pn] += [k]\n",
    "            else:\n",
    "                pdict[pn] = [k]\n",
    "\n",
    "print(f\"number of Parent Company's found in articles: {len(pdict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping Arista Networks Inc|Arista Networks Inc\n",
      "skipping Match Group Inc|Match Group Inc\n",
      "skipping Baker Hughes Co|Baker Hughes Co\n"
     ]
    }
   ],
   "source": [
    "# store the articles containing the company pairs in a dict -> This is like an inverse index \n",
    "cpairs = {}\n",
    "for i, _ in enumerate(pdict.items()):\n",
    "    pn, pv = _\n",
    "    # for each parent get the Suppliers\n",
    "    # TODO: loosen this to get all company (i.e. include Customers)\n",
    "    cnames = vc.loc[(vc[\"Parent Name\"] == pn) & (vc[\"Relationship\"] == \"Supplier\"),\n",
    "                    \"Company Name\"].unique()\n",
    "    # for each company, check the articles the parent name is mentioned in\n",
    "    for cn in cnames:\n",
    "        # company is a supplier and customer to itself - skip\n",
    "        if pn == cn:\n",
    "            print(f\"skipping {pn}|{cn}\")\n",
    "            continue\n",
    "\n",
    "        pair = f\"{pn}|{cn}\"\n",
    "        for k in pv:\n",
    "            # if company is also in article store that\n",
    "            if cn in articles[k][\"names_in_text\"]:\n",
    "                if pair in cpairs:\n",
    "                    cpairs[pair] += [k]\n",
    "                else:\n",
    "                    cpairs[pair] = [k]\n",
    "\n",
    "# find the pair that is mentioned the most\n",
    "cpair_count = [(k, len(v)) for k, v in cpairs.items()]\n",
    "cpair_count = pd.DataFrame(cpair_count, columns=[\"pair\", \"count\"])\n",
    "cpair_count.sort_values(\"count\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>Ford Motor Co|General Motors Co</td>\n",
       "      <td>1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>General Motors Co|Ford Motor Co</td>\n",
       "      <td>1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>Apple Inc|Microsoft Corp</td>\n",
       "      <td>1307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>Apple Inc|Alphabet Inc</td>\n",
       "      <td>892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Alphabet Inc|Amazon.com Inc</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>DISH Network Corp|Dell Technologies Inc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>DISH Network Corp|DISH DBS Corp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>DISH Network Corp|ViacomCBS Inc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>DISH Network Corp|Sprint Corp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>NortonLifeLock Inc|Cisco Systems Inc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1576 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         pair  count\n",
       "1105          Ford Motor Co|General Motors Co   1317\n",
       "1361          General Motors Co|Ford Motor Co   1317\n",
       "836                  Apple Inc|Microsoft Corp   1307\n",
       "826                    Apple Inc|Alphabet Inc    892\n",
       "388               Alphabet Inc|Amazon.com Inc    783\n",
       "...                                       ...    ...\n",
       "1186  DISH Network Corp|Dell Technologies Inc      1\n",
       "1185          DISH Network Corp|DISH DBS Corp      1\n",
       "1183          DISH Network Corp|ViacomCBS Inc      1\n",
       "1182            DISH Network Corp|Sprint Corp      1\n",
       "1143     NortonLifeLock Inc|Cisco Systems Inc      1\n",
       "\n",
       "[1576 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpair_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21021</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Safran SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21022</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Pratt &amp; Whitney Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21023</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Air Lease Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21024</th>\n",
       "      <td>Customer</td>\n",
       "      <td>China Eastern Airlines Corp Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21025</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Egyptair Holding Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21026</th>\n",
       "      <td>Supplier</td>\n",
       "      <td>GE Aviation Systems Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21081</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Virgin America Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21082</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Airbus SAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21083</th>\n",
       "      <td>Supplier</td>\n",
       "      <td>Safran SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21084</th>\n",
       "      <td>Supplier</td>\n",
       "      <td>Fadec Alliance LLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21085</th>\n",
       "      <td>Customer</td>\n",
       "      <td>American Airlines Group Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21230</th>\n",
       "      <td>Customer</td>\n",
       "      <td>le Bourget SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21231</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Airbus SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21232</th>\n",
       "      <td>Supplier</td>\n",
       "      <td>Kaydon Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21295</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Boeing Co</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21296</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Tui Travel Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21297</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Delta Air Lines Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21298</th>\n",
       "      <td>Customer</td>\n",
       "      <td>Easyjet PLC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Relationship                     Company Name\n",
       "21021     Customer                        Safran SA\n",
       "21022     Customer               Pratt & Whitney Co\n",
       "21023     Customer                   Air Lease Corp\n",
       "21024     Customer  China Eastern Airlines Corp Ltd\n",
       "21025     Customer              Egyptair Holding Co\n",
       "21026     Supplier          GE Aviation Systems Ltd\n",
       "21081     Customer               Virgin America Inc\n",
       "21082     Customer                       Airbus SAS\n",
       "21083     Supplier                        Safran SA\n",
       "21084     Supplier               Fadec Alliance LLC\n",
       "21085     Customer      American Airlines Group Inc\n",
       "21230     Customer                    le Bourget SA\n",
       "21231     Customer                        Airbus SE\n",
       "21232     Supplier                      Kaydon Corp\n",
       "21295     Customer                        Boeing Co\n",
       "21296     Customer                   Tui Travel Ltd\n",
       "21297     Customer              Delta Air Lines Inc\n",
       "21298     Customer                      Easyjet PLC"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for 'CFM International Inc' as an example if companies are customer and supplier -> They are: Look at Safran SA\n",
    "\n",
    "vc.loc[(vc[\"Parent Name\"] == 'CFM International Inc'),\n",
    "                    [\"Relationship\",\"Company Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Name to Short Name Conversion to increase number of matches in articles\n",
    "-> Replaces this with call from Long/Short Name Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# get name suffixes - in an attempt to make a 'short' name that often gets referenced in article\n",
    "# ---\n",
    "\n",
    "# TODO: short_name_map needs to be reviewed!, preferable to use some NLP package (spacy?)\n",
    "# This is pretty hard coded list of company name 'suffixes'\n",
    "# - some of these were taken by counting suffixes occrances, removing those and repeating\n",
    "# - others were just a gues\n",
    "suffixes = ['Inc', 'Corp', 'Ltd', 'Co', 'PLC', 'SA', 'AG', 'LLC', 'NV', 'SE',\n",
    "            'ASA', 'Bhd', 'SpA', 'Association', 'Aerospace', 'AB', 'Oyj'] + \\\n",
    "            ['Co', 'Holdings', 'Group', 'Technologies', 'International',\n",
    "            'Systems', 'Energy', 'Communications', 'Airlines', 'Motor',\n",
    "            'Technology', 'Oil', 'Motors', 'Corp', 'Industries', 'Steel',\n",
    "            'Holding', 'Airways', 'Aviation', 'Automotive', 'Networks',\n",
    "            'Electronics', 'Digital', 'BP', 'Electric', 'Aircraft',\n",
    "            'US', 'Mobile', 'Software', 'Broadcom', 'Brands',\n",
    "            'Service', 'Semiconductor', 'Petroleum'] + \\\n",
    "            ['Platforms', 'Precision', 'Industry', 'AeroSystems', 'Media', 'Petrochemical']\n",
    "\n",
    "\n",
    "\n",
    "#  'International Business Machines Corp' -> 'IBM'\n",
    "# 'News Corp' -> 'News Corp'\n",
    "# NOTE: if it starts with air it needs two words\n",
    "\n",
    "all_names = np.unique(np.concatenate([v[\"names_in_text\"] for k, v in articles.items()]))\n",
    "\n",
    "short_name = pd.DataFrame([(n, remove_suffix(n, suffixes)) for n in all_names],\n",
    "                            columns=[\"name\", \"short\"])\n",
    "# look at longer names\n",
    "short_name[\"len\"] = [len(n) for n in short_name[\"short\"]]\n",
    "short_name.sort_values(\"len\", ascending=False, inplace=True)\n",
    "\n",
    "# making a mapping dictionary\n",
    "short_name_map = {i[0]: i[1] for i in zip(short_name[\"name\"], short_name[\"short\"])}\n",
    "\n",
    "# HARDCODED!\n",
    "short_name_map['International Business Machines Corp'] = \"IBM\"\n",
    "short_name_map['News Corp'] = 'News Corp'\n",
    "short_name_map[\"Amazon.com Inc\"] = \"Amazon\"\n",
    "short_name_map[\"General Electric Co\"] = \"GE\"\n",
    "short_name_map[\"Lockheed Martin Corp\"] = \"Lockheed\"\n",
    "\n",
    "c_count[\"short_name\"] = c_count[\"name\"].map(short_name_map)\n",
    "\n",
    "# TODO: should check content to determine the which (short names?) are duplicated\n",
    "\n",
    "# -----\n",
    "# replace long names with short ones\n",
    "# -----\n",
    "\n",
    "# replace the long names with short names\n",
    "# - this is done because the short names are often used more\n",
    "keys = list(articles.keys())\n",
    "for k in keys:\n",
    "    a = articles[k]\n",
    "    for n in a['names_in_text']:\n",
    "        a[\"maintext\"] = re.sub(n, short_name_map[n], a[\"maintext\"])\n",
    "        # HARDCODE: replace ’ with ' - just a guess to try to deal with them\n",
    "        a[\"maintext\"] = re.sub(\"’\", \"'\", a[\"maintext\"])\n",
    "\n",
    "    a[\"short_names_in_text\"] = [short_name_map[n] for n in a[\"names_in_text\"]]\n",
    "    articles[k] = a\n",
    "\n",
    "# TODO: remove \"F2 percent\" ? LMT;-PCTCHNG:2} ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>in_articles</th>\n",
       "      <th>short_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>5508</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>2870</td>\n",
       "      <td>Amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Boeing Co</td>\n",
       "      <td>2689</td>\n",
       "      <td>Boeing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>Microsoft Corp</td>\n",
       "      <td>2399</td>\n",
       "      <td>Microsoft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>General Motors Co</td>\n",
       "      <td>2084</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Key Safety Systems Inc</td>\n",
       "      <td>1</td>\n",
       "      <td>Key Safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Cypress Semiconductor Corp</td>\n",
       "      <td>1</td>\n",
       "      <td>Cypress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Pilipinas Shell Petroleum Corp</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilipinas Shell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Pfizer Inc</td>\n",
       "      <td>1</td>\n",
       "      <td>Pfizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>China Tower Corp Ltd</td>\n",
       "      <td>1</td>\n",
       "      <td>China Tower</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>747 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name  in_articles       short_name\n",
       "56                        Apple Inc         5508            Apple\n",
       "46                   Amazon.com Inc         2870           Amazon\n",
       "100                       Boeing Co         2689           Boeing\n",
       "415                  Microsoft Corp         2399        Microsoft\n",
       "260               General Motors Co         2084          General\n",
       "..                              ...          ...              ...\n",
       "351          Key Safety Systems Inc            1       Key Safety\n",
       "179      Cypress Semiconductor Corp            1          Cypress\n",
       "508  Pilipinas Shell Petroleum Corp            1  Pilipinas Shell\n",
       "504                      Pfizer Inc            1           Pfizer\n",
       "147            China Tower Corp Ltd            1      China Tower\n",
       "\n",
       "[747 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mapping from Long to Short Names\n",
    "c_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Intel Corp', 'Samsung Electronics Co Ltd', 'Apple Inc']\n",
      "['Intel', 'Samsung', 'Apple']\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "print(articles[list(articles.keys())[0]]['names_in_text'])\n",
    "print(articles[list(articles.keys())[0]]['short_names_in_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating example sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Potentially add special tokenization rule\n",
    "# nlp.tokenizer.add_special_case(u'shell',\n",
    "#     [\n",
    "#         {\n",
    "#             ORTH: u'shell',\n",
    "#             LEMMA: u'shell',\n",
    "#             POS: u'NOUN'}\n",
    "#      ])\n",
    "\n",
    "\n",
    "# #Alternatively\n",
    "# # Add special case rule\n",
    "# special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
    "# nlp.tokenizer.add_special_case(\"gimme\", special_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4595\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "100 / 4595\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "200 / 4595\n",
      "300 / 4595\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "400 / 4595\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mitsubishi', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "500 / 4595\n",
      "600 / 4595\n",
      "700 / 4595\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "800 / 4595\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "900 / 4595\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "1000 / 4595\n",
      "('Tata', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "1100 / 4595\n",
      "('Honeywell', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "1200 / 4595\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "('Mobil', ' not found, most likely it was included in the other company name')\n",
      "1300 / 4595\n",
      "1400 / 4595\n",
      "('Singapore', ' not found, most likely it was included in the other company name')\n",
      "1500 / 4595\n",
      "1600 / 4595\n",
      "1700 / 4595\n",
      "1800 / 4595\n",
      "1900 / 4595\n",
      "2000 / 4595\n",
      "2100 / 4595\n",
      "2200 / 4595\n",
      "2300 / 4595\n",
      "2400 / 4595\n",
      "2500 / 4595\n",
      "2600 / 4595\n",
      "2700 / 4595\n",
      "2800 / 4595\n",
      "2900 / 4595\n",
      "3000 / 4595\n",
      "('Singapore', ' not found, most likely it was included in the other company name')\n",
      "3100 / 4595\n",
      "3200 / 4595\n",
      "3300 / 4595\n",
      "3400 / 4595\n",
      "3500 / 4595\n",
      "3600 / 4595\n",
      "3700 / 4595\n",
      "3800 / 4595\n",
      "3900 / 4595\n",
      "4000 / 4595\n",
      "4100 / 4595\n",
      "4200 / 4595\n",
      "4300 / 4595\n",
      "4400 / 4595\n",
      "4500 / 4595\n"
     ]
    }
   ],
   "source": [
    "####VERSION WITH NEGATIVE EXAMPLES ALSO\n",
    "\n",
    "# ----\n",
    "# spacy\n",
    "# ----\n",
    "\n",
    "# TODO: perhaps want to get rid of sentences that start with \\nFILE PHOTO\n",
    "# - and end with \\nFILE PHOTO\n",
    "\n",
    "\n",
    "# https://spacy.io/usage/spacy-101#annotations\n",
    "\n",
    "# requires:$ python -m spacy download en_core_web_md\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# ----\n",
    "# Extending entity ruler -> NOT USED AT THIS POINT ANYMORE (Use direct matching of company short name instead )\n",
    "# ----\n",
    "# https://spacy.io/usage/rule-based-matching#entityruler\n",
    "\n",
    "ruler = nlp.add_pipe('entity_ruler')\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": v} for v in short_name_map.values()]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# store results in a list\n",
    "out = []\n",
    "#Create an error log\n",
    "errorlog = {}\n",
    "# increment over each of the articles that reference supply\n",
    "for ii, k in enumerate(supply_articles):\n",
    "#for ii, k in enumerate([\"d70b8e93655d2bcfe4d17991e8d6a3a6ef5b8bd6b86d55e6a7ebd5ae5b1b637e\"]):\n",
    "    if ii % 100 == 0:\n",
    "        print(f\"{ii} / {len(supply_articles)}\")\n",
    "    \n",
    "    #This create all possible pairs that are included in the article \n",
    "    combinations = itertools.combinations(articles[k][\"names_in_text\"],2)   \n",
    "    pairs_list = [f\"{combination[0]}|{combination[1]}\" for combination in combinations]\n",
    "    \n",
    "    #cps lists all the pairs that are included in the article that are also in the knowledge base -> Will use this to create a label for the article\n",
    "    cps = [cp for cp, v in cpairs.items() if k in v]\n",
    "    #cps includes all paris from the KB mentioned in the article. cps_mirror includes all the reversed pairs\n",
    "    #This is done so that the order in which all possible pairs were created does not matter when we generate the label\n",
    "    cps_mirror = cps + [f\"{cp.split('|')[::-1][0]}|{cp.split('|')[::-1][1]}\" for cp in cps]\n",
    "\n",
    "    text = articles[k][\"maintext\"]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    #Split article into sentences\n",
    "    sent_list = list(doc.sents)\n",
    "    sent_start = np.array([sent.start_char for sent in doc.sents])\n",
    "    sent_end = np.array([sent.end_char for sent in doc.sents])\n",
    "    \n",
    "    # for each pair, get the short name\n",
    "    for cp in pairs_list:\n",
    "        #TODO: Check if better to use custom tokenizier\n",
    "        # long names\n",
    "        ln = cp.split(\"|\")\n",
    "        # find relevant short name using the mapping to match names in text\n",
    "        sn = [short_name_map[l] for l in ln]\n",
    "\n",
    "        # Generate a label for the pair that is used forall senteces\n",
    "        label = 1 if cp in cps_mirror else 0\n",
    "\n",
    "        # find the locations where entity 'a' is mentioned\n",
    "        # a = [(m.start(), m.end()) for m in re.finditer(sn[0], articles[k][\"maintext\"])]\n",
    "\n",
    "        try:\n",
    "            #Finding short names using regex over full doc\n",
    "            #First find all mentions of a and b\n",
    "            a_unf = []\n",
    "            expression_a = sn[0]\n",
    "            for match in re.finditer(expression_a, doc.text):\n",
    "                a_unf.append(match)\n",
    "            \n",
    "            b_unf = []\n",
    "            expression_b = sn[1]\n",
    "            for match in re.finditer(expression_b, doc.text):\n",
    "                b_unf.append(match)\n",
    "\n",
    "            #Filter out cases where one name encompasses another\n",
    "            a = []\n",
    "            for span_a in a_unf:\n",
    "                count = 0\n",
    "                for span_b in b_unf:\n",
    "                    #Check if the matches for a are included in the matches found in b, if yes ignore those -> Example: comp a Suzuki's match is included in comp b Maruti Suzuki India's match\n",
    "                    if span_a.span()[0] >= span_b.span()[0] and span_a.span()[1] <= span_b.span()[1]:\n",
    "                        count +=1\n",
    "                if count==0:\n",
    "                    a.append(span_a.span()) \n",
    "            a = np.unique(np.array(a)).reshape(-1,2)\n",
    "\n",
    "            b = []\n",
    "            for span_b in b_unf:\n",
    "                count = 0\n",
    "                for span_a in a_unf:\n",
    "                    #Check if the matches for b are included in the matches found in a, if yes ignore those\n",
    "                    if span_b.span()[0] >= span_a.span()[0] and span_b.span()[1] <= span_a.span()[1]:\n",
    "                        count +=1\n",
    "                if count==0:\n",
    "                    b.append(span_b.span())\n",
    "            b = np.unique(np.array(b)).reshape(-1,2)\n",
    "\n",
    "            #Check if either a or b is empty\n",
    "            if len(np.squeeze(a)) == 0:\n",
    "                raise Exception(sn[0],\" not found, most likely it was included in the other company name\")\n",
    "            if len(np.squeeze(b)) == 0:\n",
    "                raise Exception(sn[1],\" not found, most likely it was included in the other company name\")\n",
    "\n",
    "            # find points in the text to connect the two, via sentence\n",
    "            # storing long names instead of short names\n",
    "            start_pts, names = get_start_end(a=a[:,0], b=b[:,0], aname=ln[0], bname=ln[1])\n",
    "            end_pts, names = get_start_end(a[:,1], b[:,1], aname=ln[0], bname=ln[1])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if k in errorlog.keys():\n",
    "                errorlog[k] += [cp]\n",
    "            else:\n",
    "                errorlog[k] = [cp]\n",
    "\n",
    "        assert len(start_pts) == len(end_pts), \\\n",
    "            \"starting points and end points not to the, expect them to be\"\n",
    "\n",
    "        # get the full sentence\n",
    "        for i, sp in enumerate(start_pts):\n",
    "            nme = names[i]\n",
    "            ep = end_pts[i]\n",
    "            # find the start sentence location\n",
    "            # - by taking the start of the first entity, identify the sentences\n",
    "            # - where that is before the end and take the maximum\n",
    "            # TODO: this should be double checked/validated\n",
    "            sloc = np.argmax(sp[0] < sent_end)\n",
    "            # end sentence location\n",
    "            eloc = np.argmax(sp[1] < sent_end)\n",
    "\n",
    "            # s = sent_list[0]\n",
    "            # s.char_span(start_idx=0, end_idx=200)\n",
    "\n",
    "            # TODO: should there be a limit in the distance / number\n",
    "            #  of sentences inbetween?\n",
    "            # for now require difference to be less than equal to two\n",
    "            # NOTE: some sentence could just be \\n\n",
    "            if eloc - sloc <= 3:\n",
    "                # 'left' sentence starts\n",
    "                left_sent_start = sent_list[sloc].start_char\n",
    "                # take to where the first entity start\n",
    "                left = text[left_sent_start: sp[0]]\n",
    "                # middle is from end of first entity to start of next\n",
    "                middle = text[ep[0]:sp[1]]\n",
    "                # right is from end of second entity to the end of that sentence\n",
    "                right = text[ep[1]: sent_list[eloc].end_char]\n",
    "\n",
    "                # full_sentence = left + nme[0] + middle + nme[1] + right\n",
    "\n",
    "                # store results in a list\n",
    "                out.append([nme[0], nme[1], left, middle, right, k, label])\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(out, columns=[\"entity1\", \"entity2\", \"left\", \"middle\", \"right\", \"article\",\"label\"])\n",
    "\n",
    "df.to_csv(get_data_path(\"example_inputs_pos_and_neg.tsv\"), sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity1</th>\n",
       "      <th>entity2</th>\n",
       "      <th>left</th>\n",
       "      <th>middle</th>\n",
       "      <th>right</th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Viavi Solutions Inc</td>\n",
       "      <td>Finisar Corp</td>\n",
       "      <td>\\nAccording to parts manufacturers</td>\n",
       "      <td>,</td>\n",
       "      <td>and Ams AG, bottlenecks on key parts will mea...</td>\n",
       "      <td>b87f0cf775e9bf0419dd5298dd8339166518f0dd60717b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>* 10-yr yield hits 3 pct for first time in fou...</td>\n",
       "      <td>,</td>\n",
       "      <td>pull indexes down\\n*</td>\n",
       "      <td>90ce5e433c2f36e596ecc077cc8d101da1e52fbbfe8f60...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>\\nTechnology and industrial stocks weighed on ...</td>\n",
       "      <td>, Facebook Inc,</td>\n",
       "      <td>and Caterpillar all falling more than 3.5 per...</td>\n",
       "      <td>90ce5e433c2f36e596ecc077cc8d101da1e52fbbfe8f60...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3M Co</td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>\\nTechnology and industrial stocks weighed on ...</td>\n",
       "      <td>and Caterpillar all falling more than 3.5 per...</td>\n",
       "      <td>shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...</td>\n",
       "      <td>90ce5e433c2f36e596ecc077cc8d101da1e52fbbfe8f60...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>\\n</td>\n",
       "      <td>shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...</td>\n",
       "      <td>was the biggest drag on the Dow Jones Industr...</td>\n",
       "      <td>90ce5e433c2f36e596ecc077cc8d101da1e52fbbfe8f60...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23208</th>\n",
       "      <td>Samsung Electronics Co Ltd</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>But it is priced cheaper than comparable iPad ...</td>\n",
       "      <td>Galaxy Tab models at 1,499 yuan (142 pound)) ...</td>\n",
       "      <td>'s iPad,\" Xiaomi's founder Lei Jun said at the...</td>\n",
       "      <td>314f65ec6b5b415077196cf630f4610077090b206a215c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23209</th>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Samsung Electronics Co Ltd</td>\n",
       "      <td>\\n\"Xiaomi has a different business model than</td>\n",
       "      <td>or</td>\n",
       "      <td>,\" said Ben Thompson, founder of Stratechery.c...</td>\n",
       "      <td>314f65ec6b5b415077196cf630f4610077090b206a215c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23210</th>\n",
       "      <td>Samsung Electronics Co Ltd</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>\\n\"Xiaomi has a different business model than ...</td>\n",
       "      <td>,\" said Ben Thompson, founder of Stratechery.c...</td>\n",
       "      <td>makes it profits on the hardware, while Xiaom...</td>\n",
       "      <td>314f65ec6b5b415077196cf630f4610077090b206a215c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23211</th>\n",
       "      <td>Sharp Corp</td>\n",
       "      <td>Japan Display Inc</td>\n",
       "      <td>\\nXiaomi's success also is creating new opport...</td>\n",
       "      <td>and</td>\n",
       "      <td>, as they bolster their offerings of high-spec...</td>\n",
       "      <td>314f65ec6b5b415077196cf630f4610077090b206a215c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23212</th>\n",
       "      <td>Japan Display Inc</td>\n",
       "      <td>Sharp Corp</td>\n",
       "      <td>\\nXiaomi's success also is creating new opport...</td>\n",
       "      <td>, as they bolster their offerings of high-spec...</td>\n",
       "      <td>as a supplier and said it had provided the di...</td>\n",
       "      <td>314f65ec6b5b415077196cf630f4610077090b206a215c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23213 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          entity1                     entity2  \\\n",
       "0             Viavi Solutions Inc                Finisar Corp   \n",
       "1                    Alphabet Inc                       3M Co   \n",
       "2                    Alphabet Inc                       3M Co   \n",
       "3                           3M Co                Alphabet Inc   \n",
       "4                    Alphabet Inc                       3M Co   \n",
       "...                           ...                         ...   \n",
       "23208  Samsung Electronics Co Ltd                   Apple Inc   \n",
       "23209                   Apple Inc  Samsung Electronics Co Ltd   \n",
       "23210  Samsung Electronics Co Ltd                   Apple Inc   \n",
       "23211                  Sharp Corp           Japan Display Inc   \n",
       "23212           Japan Display Inc                  Sharp Corp   \n",
       "\n",
       "                                                    left  \\\n",
       "0                    \\nAccording to parts manufacturers    \n",
       "1      * 10-yr yield hits 3 pct for first time in fou...   \n",
       "2      \\nTechnology and industrial stocks weighed on ...   \n",
       "3      \\nTechnology and industrial stocks weighed on ...   \n",
       "4                                                     \\n   \n",
       "...                                                  ...   \n",
       "23208  But it is priced cheaper than comparable iPad ...   \n",
       "23209     \\n\"Xiaomi has a different business model than    \n",
       "23210  \\n\"Xiaomi has a different business model than ...   \n",
       "23211  \\nXiaomi's success also is creating new opport...   \n",
       "23212  \\nXiaomi's success also is creating new opport...   \n",
       "\n",
       "                                                  middle  \\\n",
       "0                                                     ,    \n",
       "1                                                     ,    \n",
       "2                                       , Facebook Inc,    \n",
       "3       and Caterpillar all falling more than 3.5 per...   \n",
       "4       shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...   \n",
       "...                                                  ...   \n",
       "23208   Galaxy Tab models at 1,499 yuan (142 pound)) ...   \n",
       "23209                                                or    \n",
       "23210  ,\" said Ben Thompson, founder of Stratechery.c...   \n",
       "23211                                               and    \n",
       "23212  , as they bolster their offerings of high-spec...   \n",
       "\n",
       "                                                   right  \\\n",
       "0       and Ams AG, bottlenecks on key parts will mea...   \n",
       "1                                   pull indexes down\\n*   \n",
       "2       and Caterpillar all falling more than 3.5 per...   \n",
       "3       shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...   \n",
       "4       was the biggest drag on the Dow Jones Industr...   \n",
       "...                                                  ...   \n",
       "23208  's iPad,\" Xiaomi's founder Lei Jun said at the...   \n",
       "23209  ,\" said Ben Thompson, founder of Stratechery.c...   \n",
       "23210   makes it profits on the hardware, while Xiaom...   \n",
       "23211  , as they bolster their offerings of high-spec...   \n",
       "23212   as a supplier and said it had provided the di...   \n",
       "\n",
       "                                                 article  label  \n",
       "0      b87f0cf775e9bf0419dd5298dd8339166518f0dd60717b...      0  \n",
       "1      90ce5e433c2f36e596ecc077cc8d101da1e52fbbfe8f60...      0  \n",
       "2      90ce5e433c2f36e596ecc077cc8d101da1e52fbbfe8f60...      0  \n",
       "3      90ce5e433c2f36e596ecc077cc8d101da1e52fbbfe8f60...      0  \n",
       "4      90ce5e433c2f36e596ecc077cc8d101da1e52fbbfe8f60...      0  \n",
       "...                                                  ...    ...  \n",
       "23208  314f65ec6b5b415077196cf630f4610077090b206a215c...      0  \n",
       "23209  314f65ec6b5b415077196cf630f4610077090b206a215c...      0  \n",
       "23210  314f65ec6b5b415077196cf630f4610077090b206a215c...      0  \n",
       "23211  314f65ec6b5b415077196cf630f4610077090b206a215c...      0  \n",
       "23212  314f65ec6b5b415077196cf630f4610077090b206a215c...      0  \n",
       "\n",
       "[23213 rows x 7 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5819583853874983"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ratio between positive and negative examples\n",
    "df[\"label\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Articles in final format that can be used by Stanford Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe in the form required by the Coprus class\n",
    "articles_final = pd.DataFrame()\n",
    "#Bring DF in shape expected by Corpus class\n",
    "articles_final.loc[:,[\"entity1\",\"entity2\",\"left\",\"entity1\",'middle','entity2','right',\"left\",\"entity1\",'middle','entity2','right']] = df.loc[:,[\"entity1\",\"entity2\",\"left\",\"entity1\",'middle','entity2','right',\"left\",\"entity1\",'middle','entity2','right']]\n",
    "#Remove \\n as this trips up the Corpus class\n",
    "articles_final.replace(\"\\n\",\"\",inplace=True)\n",
    "articles_final.replace('(\\n)','',regex=True,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity1</th>\n",
       "      <th>entity2</th>\n",
       "      <th>left</th>\n",
       "      <th>entity1</th>\n",
       "      <th>middle</th>\n",
       "      <th>entity2</th>\n",
       "      <th>right</th>\n",
       "      <th>left</th>\n",
       "      <th>entity1</th>\n",
       "      <th>middle</th>\n",
       "      <th>entity2</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Viavi Solutions Inc</td>\n",
       "      <td>Finisar Corp</td>\n",
       "      <td>According to parts manufacturers</td>\n",
       "      <td>Viavi Solutions Inc</td>\n",
       "      <td>,</td>\n",
       "      <td>Finisar Corp</td>\n",
       "      <td>and Ams AG, bottlenecks on key parts will mea...</td>\n",
       "      <td>According to parts manufacturers</td>\n",
       "      <td>Viavi Solutions Inc</td>\n",
       "      <td>,</td>\n",
       "      <td>Finisar Corp</td>\n",
       "      <td>and Ams AG, bottlenecks on key parts will mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>* 10-yr yield hits 3 pct for first time in fou...</td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>,</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>pull indexes down*</td>\n",
       "      <td>* 10-yr yield hits 3 pct for first time in fou...</td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>,</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>pull indexes down*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>Technology and industrial stocks weighed on th...</td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>, Facebook Inc,</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>and Caterpillar all falling more than 3.5 per...</td>\n",
       "      <td>Technology and industrial stocks weighed on th...</td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>, Facebook Inc,</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>and Caterpillar all falling more than 3.5 per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3M Co</td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>Technology and industrial stocks weighed on th...</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>and Caterpillar all falling more than 3.5 per...</td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...</td>\n",
       "      <td>Technology and industrial stocks weighed on th...</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>and Caterpillar all falling more than 3.5 per...</td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>3M Co</td>\n",
       "      <td></td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>was the biggest drag on the Dow Jones Industr...</td>\n",
       "      <td></td>\n",
       "      <td>Alphabet Inc</td>\n",
       "      <td>shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>was the biggest drag on the Dow Jones Industr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               entity1       entity2  \\\n",
       "0  Viavi Solutions Inc  Finisar Corp   \n",
       "1         Alphabet Inc         3M Co   \n",
       "2         Alphabet Inc         3M Co   \n",
       "3                3M Co  Alphabet Inc   \n",
       "4         Alphabet Inc         3M Co   \n",
       "\n",
       "                                                left              entity1  \\\n",
       "0                  According to parts manufacturers   Viavi Solutions Inc   \n",
       "1  * 10-yr yield hits 3 pct for first time in fou...         Alphabet Inc   \n",
       "2  Technology and industrial stocks weighed on th...         Alphabet Inc   \n",
       "3  Technology and industrial stocks weighed on th...                3M Co   \n",
       "4                                                            Alphabet Inc   \n",
       "\n",
       "                                              middle       entity2  \\\n",
       "0                                                 ,   Finisar Corp   \n",
       "1                                                 ,          3M Co   \n",
       "2                                   , Facebook Inc,          3M Co   \n",
       "3   and Caterpillar all falling more than 3.5 per...  Alphabet Inc   \n",
       "4   shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...         3M Co   \n",
       "\n",
       "                                               right  \\\n",
       "0   and Ams AG, bottlenecks on key parts will mea...   \n",
       "1                                 pull indexes down*   \n",
       "2   and Caterpillar all falling more than 3.5 per...   \n",
       "3   shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...   \n",
       "4   was the biggest drag on the Dow Jones Industr...   \n",
       "\n",
       "                                                left              entity1  \\\n",
       "0                  According to parts manufacturers   Viavi Solutions Inc   \n",
       "1  * 10-yr yield hits 3 pct for first time in fou...         Alphabet Inc   \n",
       "2  Technology and industrial stocks weighed on th...         Alphabet Inc   \n",
       "3  Technology and industrial stocks weighed on th...                3M Co   \n",
       "4                                                            Alphabet Inc   \n",
       "\n",
       "                                              middle       entity2  \\\n",
       "0                                                 ,   Finisar Corp   \n",
       "1                                                 ,          3M Co   \n",
       "2                                   , Facebook Inc,          3M Co   \n",
       "3   and Caterpillar all falling more than 3.5 per...  Alphabet Inc   \n",
       "4   shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...         3M Co   \n",
       "\n",
       "                                               right  \n",
       "0   and Ams AG, bottlenecks on key parts will mea...  \n",
       "1                                 pull indexes down*  \n",
       "2   and Caterpillar all falling more than 3.5 per...  \n",
       "3   shares fell {GOOGL.O;-PCTCHNG:2}77 percent, e...  \n",
       "4   was the biggest drag on the Dow Jones Industr...  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store as TSV\n",
    "articles_final.to_csv(get_data_path(\"example_inputs_long_names_with_neg.tsv\"), sep=\"\\t\", index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to GZ file\n",
    "with open(get_data_path(\"example_inputs_long_names_with_neg.tsv\"), 'rb') as src, gzip.open(get_data_path(\"example_inputs_long_names_with_neg.tsv.gz\"), 'wb') as dst:\n",
    "    dst.writelines(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only positive examples \n",
    "# ----\n",
    "# spacy\n",
    "# ----\n",
    "\n",
    "# TODO: perhaps want to get rid of sentences that start with \\nFILE PHOTO\n",
    "# - and end with \\nFILE PHOTO\n",
    "\n",
    "\n",
    "# https://spacy.io/usage/spacy-101#annotations\n",
    "\n",
    "# requires:$ python -m spacy download en_core_web_md\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# ----\n",
    "# Extending entity ruler -> NOT USED AT THIS POINT ANYMORE (Use direct matching of company short name instead )\n",
    "# ----\n",
    "# https://spacy.io/usage/rule-based-matching#entityruler\n",
    "\n",
    "ruler = nlp.add_pipe('entity_ruler')\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": v} for v in short_name_map.values()]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# store results in a list\n",
    "out = []\n",
    "#Create an error log\n",
    "errorlog = {}\n",
    "# increment over each of the articles that reference supply\n",
    "for ii, k in enumerate(supply_articles):\n",
    "    if ii % 100 == 0:\n",
    "        print(f\"{ii} / {len(supply_articles)}\")\n",
    "    #cps is a list of company pairs that is included in the article, filtered for the given article to include at least one of the company pairs \n",
    "    cps = [cp for cp, v in cpairs.items() if k in v]\n",
    "    text = articles[k][\"maintext\"]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sent_list = list(doc.sents)\n",
    "    sent_start = np.array([sent.start_char for sent in doc.sents])\n",
    "    sent_end = np.array([sent.end_char for sent in doc.sents])\n",
    "    # for each pair, get the short name\n",
    "    for cp in cps:\n",
    "    #for cp in ['Exxon Mobil Corp|Mobil Corp',]:\n",
    "        # long names\n",
    "        ln = cp.split(\"|\")\n",
    "        sn = [short_name_map[l] for l in ln]\n",
    "\n",
    "        # find the locations where entity 'a' is mentioned\n",
    "        # a = [(m.start(), m.end()) for m in re.finditer(sn[0], articles[k][\"maintext\"])]\n",
    "\n",
    "        try:\n",
    "            #Finding short names using regex over full doc\n",
    "            a_unf = []\n",
    "            expression_a = sn[0]\n",
    "            for match in re.finditer(expression_a, doc.text):\n",
    "                a_unf.append(match)\n",
    "            #a_unf = np.array(a_unf)\n",
    "\n",
    "            b_unf = []\n",
    "            expression_b = sn[1]\n",
    "            for match in re.finditer(expression_b, doc.text):\n",
    "                b_unf.append(match)\n",
    "            #b_unf = np.array(b_unf)\n",
    "\n",
    "            a = []\n",
    "            for span_a in a_unf:\n",
    "                count = 0\n",
    "                for span_b in b_unf:\n",
    "                    #Check if the matches for a are included in the matches found in b, if yes ignore those -> Example: comp a Suzuki's match is included in comp b Maruti Suzuki India's match\n",
    "                    if span_a.span()[0] >= span_b.span()[0] and span_a.span()[1] <= span_b.span()[1]:\n",
    "                        count +=1\n",
    "                if count==0:\n",
    "                    a.append(span_a.span())\n",
    "                    \n",
    "            a = np.unique(np.array(a)).reshape(-1,2)\n",
    "\n",
    "            b = []\n",
    "            for span_b in b_unf:\n",
    "                count = 0\n",
    "                for span_a in a_unf:\n",
    "                    #Check if the matches for b are included in the matches found in a, if yes ignore those\n",
    "                    if span_b.span()[0] >= span_a.span()[0] and span_b.span()[1] <= span_a.span()[1]:\n",
    "                        count +=1\n",
    "                if count==0:\n",
    "                    b.append(span_b.span())\n",
    "                        \n",
    "            b = np.unique(np.array(b)).reshape(-1,2)\n",
    "\n",
    "            #Check if either a or b is empty\n",
    "            if len(np.squeeze(a)) == 0:\n",
    "                raise Exception(sn[0],\" not found, most likely it was included in the other company name\")\n",
    "            if len(np.squeeze(b)) == 0:\n",
    "                raise Exception(sn[1],\" not found, most likely it was included in the other company name\")\n",
    "\n",
    "            # find points in the text to connect the two, via sentence\n",
    "            start_pts, names = get_start_end(a=a[:,0], b=b[:,0], aname=sn[0], bname=sn[1])\n",
    "            end_pts, names = get_start_end(a[:,1], b[:,1], aname=sn[0], bname=sn[1])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if k in errorlog.keys():\n",
    "                errorlog[k] += [cp]\n",
    "            else:\n",
    "                errorlog[k] = [cp]\n",
    "\n",
    "        assert len(start_pts) == len(end_pts), \\\n",
    "            \"starting points and end points not to the, expect them to be\"\n",
    "\n",
    "        # get the full sentence\n",
    "        for i, sp in enumerate(start_pts):\n",
    "            nme = names[i]\n",
    "            ep = end_pts[i]\n",
    "            # find the start sentence location\n",
    "            # - by taking the start of the first entity, identify the sentences\n",
    "            # - where that is before the end and take the maximum\n",
    "            # TODO: this should be double checked/validated\n",
    "            sloc = np.argmax(sp[0] < sent_end)\n",
    "            # end sentence location\n",
    "            eloc = np.argmax(sp[1] < sent_end)\n",
    "\n",
    "            # s = sent_list[0]\n",
    "            # s.char_span(start_idx=0, end_idx=200)\n",
    "\n",
    "            # TODO: should there be a limit in the distance / number\n",
    "            #  of sentences inbetween?\n",
    "            # for now require difference to be less than equal to two\n",
    "            # NOTE: some sentence could just be \\n\n",
    "            if eloc - sloc <= 3:\n",
    "                # 'left' sentence starts\n",
    "                left_sent_start = sent_list[sloc].start_char\n",
    "                # take to where the first entity start\n",
    "                left = text[left_sent_start: sp[0]]\n",
    "                # middle is from end of first entity to start of next\n",
    "                middle = text[ep[0]:sp[1]]\n",
    "                # right is from end of second entity to the end of that sentence\n",
    "                right = text[ep[1]: sent_list[eloc].end_char]\n",
    "\n",
    "                # full_sentence = left + nme[0] + middle + nme[1] + right\n",
    "\n",
    "                # store results in a list\n",
    "                out.append([nme[0], nme[1], left, middle, right, k])\n",
    "\n",
    " \n",
    "\n",
    "df = pd.DataFrame(out, columns=[\"entity1\", \"entity2\", \"left\", \"middle\", \"right\", \"article\"])\n",
    "\n",
    "#df.to_csv(get_data_path(\"example_inputs.tsv\"), sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "[[2846 2855]]\n",
      "b\n",
      "[[4151 4157]]\n",
      "Air China\n",
      "Airbus\n"
     ]
    }
   ],
   "source": [
    "#Regex based way to match with filtering of one company name that includes another\n",
    "#Finding short names using regex over full doc\n",
    "a_unf = []\n",
    "expression_a = sn[0]\n",
    "for match in re.finditer(expression_a, doc.text):\n",
    "    a_unf.append(match)\n",
    "#a_unf = np.array(a_unf)\n",
    "\n",
    "b_unf = []\n",
    "expression_b = sn[1]\n",
    "for match in re.finditer(expression_b, doc.text):\n",
    "    b_unf.append(match)\n",
    "#b_unf = np.array(b_unf)\n",
    "\n",
    "a = []\n",
    "for span_a in a_unf:\n",
    "    count = 0\n",
    "    for span_b in b_unf:\n",
    "        #Check if the matches for a are included in the matches found in b, if yes ignore those -> Example: comp a Suzuki's match is included in comp b Maruti Suzuki India's match\n",
    "        if span_a.span()[0] >= span_b.span()[0] and span_a.span()[1] <= span_b.span()[1]:\n",
    "            count +=1\n",
    "    if count==0:\n",
    "        a.append(span_a.span())\n",
    "        \n",
    "a = np.unique(np.array(a)).reshape(-1,2)\n",
    "\n",
    "b = []\n",
    "for span_b in b_unf:\n",
    "    count = 0\n",
    "    for span_a in a_unf:\n",
    "        #Check if the matches for b are included in the matches found in a, if yes ignore those\n",
    "        if span_b.span()[0] >= span_a.span()[0] and span_b.span()[1] <= span_a.span()[1]:\n",
    "            count +=1\n",
    "    if count==0:\n",
    "        b.append(span_b.span())\n",
    "            \n",
    "b = np.unique(np.array(b)).reshape(-1,2)\n",
    "\n",
    "#Check if either a or b is empty\n",
    "if len(np.squeeze(a)) == 0:\n",
    "    raise Exception(sn[0],\" not found, most likely it was included in the other company name\")\n",
    "if len(np.squeeze(b)) == 0:\n",
    "    raise Exception(sn[1],\" not found, most likely it was included in the other company name\")\n",
    "\n",
    "#Print out what span found\n",
    "for span in a:\n",
    "    start, end = span\n",
    "    span = doc.char_span(start, end)\n",
    "    print(span)\n",
    "\n",
    "#Print out what span found\n",
    "for span in b:\n",
    "    start, end = span\n",
    "    span = doc.char_span(start, end)\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "100 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "200 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "300 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "400 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "500 / 4595\n",
      "600 / 4595\n",
      "700 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "800 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "900 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "1000 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "1100 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "1200 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "1300 / 4595\n",
      "1400 / 4595\n",
      "1500 / 4595\n",
      "1600 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "1700 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "1800 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "1900 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "2000 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "2100 / 4595\n",
      "2200 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "2300 / 4595\n",
      "2400 / 4595\n",
      "2500 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "2600 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "2700 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "2800 / 4595\n",
      "2900 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "3000 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "3100 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "3200 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "3300 / 4595\n",
      "3400 / 4595\n",
      "3500 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "3600 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "3700 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "3800 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "3900 / 4595\n",
      "4000 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "4100 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "4200 / 4595\n",
      "4300 / 4595\n",
      "4400 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "4500 / 4595\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "too many indices for array: array is 1-dimensional, but 2 were indexed\n"
     ]
    }
   ],
   "source": [
    "#Old Version with double-exception (first spacy NER and then regex match)\n",
    "# ----\n",
    "# spacy\n",
    "# ----\n",
    "\n",
    "# TODO: perhaps want to get rid of sentences that start with \\nFILE PHOTO\n",
    "# - and end with \\nFILE PHOTO\n",
    "\n",
    "\n",
    "# https://spacy.io/usage/spacy-101#annotations\n",
    "\n",
    "# requires:$ python -m spacy download en_core_web_md\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# https://spacy.io/usage/rule-based-matching#entityruler\n",
    "ruler = nlp.add_pipe('entity_ruler')\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": v} for v in short_name_map.values()]\n",
    "ruler.add_patterns(patterns)\n",
    "# print(json.dumps(patterns, indent=4))\n",
    "\n",
    "# for articles that mention supply (or similar)\n",
    "# - found the company pairs\n",
    "\n",
    "# store results in a list\n",
    "out = []\n",
    "investigate = []\n",
    "\n",
    "#Create an error log\n",
    "errorlog = {}\n",
    "# increment over each of the articles that reference supply\n",
    "#### Why not loop over cpairs because there we already have the articles identified??????????\n",
    "for ii, k in enumerate(supply_articles):\n",
    "#for ii, k in enumerate(['2731ed442c344ba3266e37c8ca26a099029dc8e94320a5722e3012d88c4b97e0']):\n",
    "\n",
    "    if ii % 100 == 0:\n",
    "        print(f\"{ii} / {len(supply_articles)}\")\n",
    "    cps = [cp for cp, v in cpairs.items() if k in v]\n",
    "    text = articles[k][\"maintext\"]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sent_list = list(doc.sents)\n",
    "    sent_start = np.array([sent.start_char for sent in doc.sents])\n",
    "    sent_end = np.array([sent.end_char for sent in doc.sents])\n",
    "    # for each pair, get the short name\n",
    "    for cp in cps:\n",
    "        # long names\n",
    "        ln = cp.split(\"|\")\n",
    "        sn = [short_name_map[l] for l in ln]\n",
    "\n",
    "        # find the locations where entity 'a' is mentioned\n",
    "        # a = [(m.start(), m.end()) for m in re.finditer(sn[0], articles[k][\"maintext\"])]\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # using the entity recognition\n",
    "            # a = [(ent.start_char, ent.end_char) for ent in doc.ents if ent.text == sn[0]]\n",
    "            # TODO: review if can get deal with apostrophes on entity names through spacy\n",
    "            #  i.e. identify Lockheed Martin instead Lockheed Martin's as an entity\n",
    "            # NOTE: this will sometimes not even work: got 'Lockheed' 'Martin' as to separate entities\n",
    "            a = [(ent.start_char, ent.end_char) for ent in doc.ents if re.search(f\"^{sn[0]}\", ent.text)]\n",
    "            a = np.array(a)\n",
    "\n",
    "            # find the locations where entity 'b' is mentioned\n",
    "            # b = [(m.start(), m.end())  for m in re.finditer(sn[1], articles[k][\"maintext\"])]\n",
    "\n",
    "            # using the entity recognition\n",
    "\n",
    "            # b = [(ent.start_char, ent.end_char) for ent in doc.ents if ent.text == sn[1]]\n",
    "            b = [(ent.start_char, ent.end_char) for ent in doc.ents if re.search(f\"^{sn[1]}\", ent.text)]\n",
    "            b = np.array(b)\n",
    "\n",
    "            # find points in the text to connect the two, via sentence\n",
    "            start_pts, names = get_start_end(a=a[:,0], b=b[:,0], aname=sn[0], bname=sn[1])\n",
    "            end_pts, names = get_start_end(a[:,1], b[:,1], aname=sn[0], bname=sn[1])\n",
    "        except Exception as e:\n",
    "            try: \n",
    "                #This error comes up if either company a or company b was not found, Example is Exxon Mobil and Mobil Corp, then Mobil does not come up separately from Exxon\n",
    "                \n",
    "                #Finding short names using regex over full doc\n",
    "                a_unf = []\n",
    "                expression_a = sn[0]\n",
    "                for match in re.finditer(expression_a, doc.text):\n",
    "                    a_unf.append(match)\n",
    "                #a_unf = np.array(a_unf)\n",
    "\n",
    "                b_unf = []\n",
    "                expression_b = sn[1]\n",
    "                for match in re.finditer(expression_b, doc.text):\n",
    "                    b_unf.append(match)\n",
    "                #b_unf = np.array(b_unf)\n",
    "\n",
    "                a = []\n",
    "                for span_a in a_unf:\n",
    "                    for span_b in b_unf:\n",
    "                        #Check if the matches for a are included in the matches found in b, if yes ignore those -> Example: comp a Suzuki's match is included in comp b Maruti Suzuki India's match\n",
    "                        if not(span_a.span()[0] >= span_b.span()[0] and span_a.span()[1] <= span_b.span()[1]):\n",
    "                            a.append(span_a.span())\n",
    "                        \n",
    "                a = np.unique(np.array(a)).reshape(-1,2)\n",
    "\n",
    "                b = []\n",
    "                for span_b in b_unf:\n",
    "                    for span_a in a_unf:\n",
    "                        #Check if the matches for b are included in the matches found in a, if yes ignore those\n",
    "                        if not(span_b.span()[0] >= span_a.span()[0] and span_b.span()[1] <= span_a.span()[1]):\n",
    "                            b.append(span_b.span())\n",
    "                            \n",
    "                b = np.unique(np.array(b)).reshape(-1,2)\n",
    "\n",
    "                #Check if either a or b is empty\n",
    "                if len(np.squeeze(a)) == 0:\n",
    "                    raise Exception(sn[0],\" not found, most likely it was included in the other company name\")\n",
    "                if len(np.squeeze(b)) == 0:\n",
    "                    raise Exception(sn[1],\" not found, most likely it was included in the other company name\")\n",
    "\n",
    "                # find points in the text to connect the two, via sentence\n",
    "                start_pts, names = get_start_end(a=a[:,0], b=b[:,0], aname=sn[0], bname=sn[1])\n",
    "                end_pts, names = get_start_end(a[:,1], b[:,1], aname=sn[0], bname=sn[1])\n",
    "            \n",
    "            except Exception as e:\n",
    "\n",
    "                print(e)\n",
    "                investigate.append([k, sn])\n",
    "                #Add error to errorlog for later inspection\n",
    "                if k in errorlog.keys():\n",
    "                    errorlog[k] += [cp]\n",
    "                else:\n",
    "                    errorlog[k] = [cp]\n",
    "\n",
    "        assert len(start_pts) == len(end_pts), \\\n",
    "            \"starting points and end points not to the, expect them to be\"\n",
    "\n",
    "        # get the full sentence\n",
    "        for i, sp in enumerate(start_pts):\n",
    "            nme = names[i]\n",
    "            ep = end_pts[i]\n",
    "            # find the start sentence location\n",
    "            # - by taking the start of the first entity, identify the sentences\n",
    "            # - where that is before the end and take the maximum\n",
    "            # TODO: this should be double checked/validated\n",
    "            sloc = np.argmax(sp[0] < sent_end)\n",
    "            # end sentence location\n",
    "            eloc = np.argmax(sp[1] < sent_end)\n",
    "\n",
    "            # s = sent_list[0]\n",
    "            # s.char_span(start_idx=0, end_idx=200)\n",
    "\n",
    "            # TODO: should there be a limit in the distance / number\n",
    "            #  of sentences inbetween?\n",
    "            # for now require difference to be less than equal to two\n",
    "            # NOTE: some sentence could just be \\n\n",
    "            if eloc - sloc <= 3:\n",
    "                # 'left' sentence starts\n",
    "                left_sent_start = sent_list[sloc].start_char\n",
    "                # take to where the first entity start\n",
    "                left = text[left_sent_start: sp[0]]\n",
    "                # middle is from end of first entity to start of next\n",
    "                middle = text[ep[0]:sp[1]]\n",
    "                # right is from end of second entity to the end of that sentence\n",
    "                right = text[ep[1]: sent_list[eloc].end_char]\n",
    "\n",
    "                # full_sentence = left + nme[0] + middle + nme[1] + right\n",
    "\n",
    "                # store results in a list\n",
    "                out.append([nme[0], nme[1], left, middle, right, k])\n",
    "            \n",
    "\n",
    "df = pd.DataFrame(out, columns=[\"entity1\", \"entity2\", \"left\", \"middle\", \"right\", \"article\"])\n",
    "\n",
    "#df.to_csv(get_data_path(\"example_inputs.tsv\"), sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 4595\n"
     ]
    }
   ],
   "source": [
    "#Original version\n",
    "# ----\n",
    "# spacy\n",
    "# ----\n",
    "\n",
    "# TODO: perhaps want to get rid of sentences that start with \\nFILE PHOTO\n",
    "# - and end with \\nFILE PHOTO\n",
    "\n",
    "\n",
    "# https://spacy.io/usage/spacy-101#annotations\n",
    "\n",
    "# requires:$ python -m spacy download en_core_web_md\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# https://spacy.io/usage/rule-based-matching#entityruler\n",
    "ruler = nlp.add_pipe('entity_ruler')\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": v} for v in short_name_map.values()]\n",
    "ruler.add_patterns(patterns)\n",
    "# print(json.dumps(patterns, indent=4))\n",
    "\n",
    "# for articles that mention supply (or similar)\n",
    "# - found the company pairs\n",
    "\n",
    "# store results in a list\n",
    "out = []\n",
    "investigate = []\n",
    "# increment over each of the articles that reference supply\n",
    "#for ii, k in enumerate(supply_articles[:100]):\n",
    "for ii, k in enumerate([\"d3f76045ec8f0d7c89edc026ef98d5c58f80534f8939279b8d8d2a12a638b108\"]):\n",
    "    if ii % 100 == 0:\n",
    "        print(f\"{ii} / {len(supply_articles)}\")\n",
    "    cps = [cp for cp, v in cpairs.items() if k in v]\n",
    "    text = articles[k][\"maintext\"]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sent_list = list(doc.sents)\n",
    "    sent_start = np.array([sent.start_char for sent in doc.sents])\n",
    "    sent_end = np.array([sent.end_char for sent in doc.sents])\n",
    "    # for each pair, get the short name\n",
    "    for cp in cps:\n",
    "    #for cp in ['Maruti Suzuki India Ltd|Suzuki Motor Corp']:\n",
    "        # long names\n",
    "        ln = cp.split(\"|\")\n",
    "        sn = [short_name_map[l] for l in ln]\n",
    "\n",
    "        # find the locations where entity 'a' is mentioned\n",
    "        # a = [(m.start(), m.end()) for m in re.finditer(sn[0], articles[k][\"maintext\"])]\n",
    "\n",
    "        try:\n",
    "            # using the entity recognition\n",
    "            # a = [(ent.start_char, ent.end_char) for ent in doc.ents if ent.text == sn[0]]\n",
    "            # TODO: review if can get deal with apostrophes on entity names through spacy\n",
    "            #  i.e. identify Lockheed Martin instead Lockheed Martin's as an entity\n",
    "            # NOTE: this will sometimes not even work: got 'Lockheed' 'Martin' as to separate entities\n",
    "            a = [(ent.start_char, ent.end_char) for ent in doc.ents if re.search(f\"^{sn[0]}\", ent.text)]\n",
    "            a = np.array(a)\n",
    "\n",
    "            # find the locations where entity 'b' is mentioned\n",
    "            # b = [(m.start(), m.end())  for m in re.finditer(sn[1], articles[k][\"maintext\"])]\n",
    "\n",
    "            # using the entity recognition\n",
    "\n",
    "            # b = [(ent.start_char, ent.end_char) for ent in doc.ents if ent.text == sn[1]]\n",
    "            b = [(ent.start_char, ent.end_char) for ent in doc.ents if re.search(f\"^{sn[1]}\", ent.text)]\n",
    "            b = np.array(b)\n",
    "\n",
    "            # find points in the text to connect the two, via sentence\n",
    "            start_pts, names = get_start_end(a=a[:,0], b=b[:,0], aname=sn[0], bname=sn[1])\n",
    "            end_pts, names = get_start_end(a[:,1], b[:,1], aname=sn[0], bname=sn[1])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            investigate.append([k, sn])\n",
    "            #assert true\n",
    "\n",
    "        assert len(start_pts) == len(end_pts), \\\n",
    "            \"starting points and end points not to the, expect them to be\"\n",
    "\n",
    "        # get the full sentence\n",
    "        for i, sp in enumerate(start_pts):\n",
    "            nme = names[i]\n",
    "            ep = end_pts[i]\n",
    "            # find the start sentence location\n",
    "            # - by taking the start of the first entity, identify the sentences\n",
    "            # - where that is before the end and take the maximum\n",
    "            # TODO: this should be double checked/validated\n",
    "            sloc = np.argmax(sp[0] < sent_end)\n",
    "            # end sentence location\n",
    "            eloc = np.argmax(sp[1] < sent_end)\n",
    "\n",
    "            # s = sent_list[0]\n",
    "            # s.char_span(start_idx=0, end_idx=200)\n",
    "\n",
    "            # TODO: should there be a limit in the distance / number\n",
    "            #  of sentences inbetween?\n",
    "            # for now require difference to be less than equal to two\n",
    "            # NOTE: some sentence could just be \\n\n",
    "            if eloc - sloc <= 3:\n",
    "                # 'left' sentence starts\n",
    "                left_sent_start = sent_list[sloc].start_char\n",
    "                # take to where the first entity start\n",
    "                left = text[left_sent_start: sp[0]]\n",
    "                # middle is from end of first entity to start of next\n",
    "                middle = text[ep[0]:sp[1]]\n",
    "                # right is from end of second entity to the end of that sentence\n",
    "                right = text[ep[1]: sent_list[eloc].end_char]\n",
    "\n",
    "                # full_sentence = left + nme[0] + middle + nme[1] + right\n",
    "\n",
    "                # store results in a list\n",
    "                out.append([nme[0], nme[1], left, middle, right, k])\n",
    "\n",
    " \n",
    "\n",
    "df = pd.DataFrame(out, columns=[\"entity1\", \"entity2\", \"left\", \"middle\", \"right\", \"article\"])\n",
    "\n",
    "#df.to_csv(get_data_path(\"example_inputs.tsv\"), sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # doc = nlp(\"A complex-example,!\")\n",
    "    # print([token.text for token in doc])\n",
    "    #\n",
    "    # a = articles[keys[1]]\n",
    "    # t0 = time.perf_counter()\n",
    "    # doc = nlp(a[\"maintext\"])\n",
    "    # t1 = time.perf_counter()\n",
    "    # # will take about ~ 11min to run fo 5500 articles\n",
    "    #\n",
    "    # print(a[\"short_names_in_text\"])\n",
    "    # print(json.dumps([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"ORG\"], indent=4))\n",
    "    # # for token in doc:\n",
    "    # #     print(token.text)\n",
    "    # print(a[\"names_in_text\"])\n",
    "    #\n",
    "    # for ent in doc.ents:\n",
    "    #     if ent.label_ == \"ORG\":\n",
    "    #         print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    #     # if ent.label_ == \"GPE\":\n",
    "        # if re.search(\"Alphabet\", ent.text):\n",
    "        #     print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "    # from spacy import displacy\n",
    "    # displacy.serve(doc, style=\"dep\")\n",
    "\n",
    "\n",
    "    # # ---\n",
    "    # # replace names with dash separated - in an attempt to deal with short names later\n",
    "    # # ---\n",
    "    #\n",
    "    # # TODO: drop this - just use spacy?\n",
    "    #\n",
    "    # all_names = np.unique(np.concatenate([v[\"names_in_text\"]\n",
    "    #                                       for k, v in articles.items()]))\n",
    "    # dash_names = {n: re.sub(\" \", \"-\", n) for n in all_names}\n",
    "    #\n",
    "    # # for each article replace the name with the dashed name\n",
    "    # # - the idea is this will help with tokenizing\n",
    "    #\n",
    "    #\n",
    "    # for k in keys:\n",
    "    #     a = articles[k]\n",
    "    #     anames = np.array(a[\"names_in_text\"])\n",
    "    #     dnames = np.array([dash_names[an] for an in anames])\n",
    "    #     # replace the longest matching first - to deal with\n",
    "    #     name_len = [len(an) for an in anames]\n",
    "    #     name_ord = np.argsort(name_len)[::-1]\n",
    "    #\n",
    "    #     maintext = a[\"maintext\"]\n",
    "    #     name_in_article = []\n",
    "    #     for an in anames[name_ord]:\n",
    "    #         if re.search(an, maintext):\n",
    "    #\n",
    "    #             name_in_article.append(an)\n",
    "    #             maintext = re.sub(an, dash_names[an], maintext)\n",
    "    #         else:\n",
    "    #             # print(f\"could not find: {an} in {k}\")\n",
    "    #             pass\n",
    "    #\n",
    "    #     a[\"names_in_text\"] = name_in_article\n",
    "    #\n",
    "    # # keep only those articles with two or more names_in_text\n",
    "    # articles = {k: v for k, v in articles.items() if len(v[\"names_in_text\"]) > 1}\n",
    "\n",
    "\n",
    "    # tmp = [a for a in articles.values() if 'News Corp' in a[\"names_in_text\"]]\n",
    "    #\n",
    "    #\n",
    "    # all_sufs = pd.DataFrame([ (n, n.split(\" \")[-1])for n in all_names],\n",
    "    #                         columns=[\"name\", \"suf\"])\n",
    "    # suf_count = pd.pivot_table(all_sufs,\n",
    "    #                            index=\"suf\",\n",
    "    #                            values=\"name\",\n",
    "    #                            aggfunc=\"count\").reset_index()\n",
    "    #\n",
    "    # suf_count.sort_values(\"name\", ascending=False, inplace=True)\n",
    "    #\n",
    "    # # drop the more common suffixes - in an attempt to make short names\n",
    "    # suf_vals = suf_count.loc[suf_count[\"name\"] >= 2, \"suf\"].values\n",
    "    # # regular expression for matching suffixes\n",
    "    # suf_regex = \"\".join([f\" {sv}$|\" for sv in suf_vals])\n",
    "    #\n",
    "    # # get the second suffix\n",
    "    # suf2 = pd.DataFrame([(n, re.sub(suf_regex, \"\", n).split(\" \")[-1])\n",
    "    #                      for n in all_names if bool(re.search(suf_regex, n))],\n",
    "    #                     columns=[\"name\", \"suf\"])\n",
    "    #\n",
    "    # suf_count2 = pd.pivot_table(suf2,\n",
    "    #                            index=\"suf\",\n",
    "    #                            values=\"name\",\n",
    "    #                            aggfunc=\"count\").reset_index()\n",
    "    # suf_count2.sort_values(\"name\", ascending=False, inplace=True)\n",
    "    #\n",
    "    # # do another suffix count\n",
    "    # suf_vals2 = suf_count2.loc[suf_count2[\"name\"] >= 2, \"suf\"].values\n",
    "    # suf_regex2 = \"\".join([f\" {sv}$|\" for sv in suf_vals2])\n",
    "    #\n",
    "    # short_names = {n: re.sub(suf_regex2, \"\", re.sub(suf_regex, \"\", n))\n",
    "    #                for n in all_names}\n",
    "    #\n",
    "    # tmp = [a for a in articles.values() if 'Meta Platforms Inc' in a[\"names_in_text\"]]\n",
    "    # tmp = [a for a in articles.values() if 'Electronic Arts Inc' in a[\"names_in_text\"]]\n",
    "\n",
    "\n",
    "    # HARDCODED: ['Exxon Mobil Corp', 'Mobil Corp'] were erronously mathced?\n",
    "    # drop_exxon_mobil = []\n",
    "    # for k in articles.keys():\n",
    "    #     if np.in1d(articles[k][\"names_in_text\"], ['Exxon Mobil Corp', 'Mobil Corp']).all():\n",
    "    #         drop_exxon_mobil.append(k)\n",
    "    #     # just remove Mobil Corp - this isn't robust\n",
    "    #     if np.in1d(['Mobil Corp'], articles[k][\"names_in_text\"]).any():\n",
    "    #         articles[k][\"names_in_text\"] = [n for n in articles[k][\"names_in_text\"] if n != \"Mobil Corp\"]\n",
    "    #\n",
    "    # for k in drop_exxon_mobil:\n",
    "    #     articles.pop(k)\n",
    "\n",
    "    # # ---\n",
    "    # # rules for getting short names\n",
    "    # # ---\n",
    "    #\n",
    "    # all_names = np.unique(np.concatenate([v[\"names_in_text\"]\n",
    "    #                                       for k, v in articles.items()]))\n",
    "    #\n",
    "    # tmp = find_all_articles_with_name(articles, 'Ryanair Holdings PLC')\n",
    "    #\n",
    "    # # Rule 1: first name\n",
    "    # # short_names = [n.split(\" \")[0] for n in all_names]\n",
    "    #\n",
    "    # # keep track of articles where did not find all names\n",
    "    # not_found_all_names = []\n",
    "    #\n",
    "    # #\n",
    "    # nlp = English()\n",
    "    # tokenizer = nlp.tokenizer\n",
    "    #\n",
    "    # # record all the short names\n",
    "    # short_name_dict = {n: np.array([]) for n in all_names}\n",
    "    #\n",
    "    # investigate = []\n",
    "    # for i, _ in enumerate(articles.items()):\n",
    "    #\n",
    "    #     stop = False\n",
    "    #     k, v = _\n",
    "    #     names = v[\"names_in_text\"]\n",
    "    #     # Rule 1: first name\n",
    "    #     short_names = {n: n.split(\" \")[0] for n in names}\n",
    "    #\n",
    "    #     # tokenize the data\n",
    "    #     # - is there a better, more correct way of doing this?\n",
    "    #     text = v['maintext']\n",
    "    #\n",
    "    #     # confirm the long names are in text\n",
    "    #     # - then remove it\n",
    "    #     for n in names:\n",
    "    #\n",
    "    #         if not bool(re.search(n, text)):\n",
    "    #             print(f\"{n} not found in text, expected to!\")\n",
    "    #             print(f\"names list: {names}\")\n",
    "    #             stop = True\n",
    "    #             break\n",
    "    #\n",
    "    #         # remove the long names\n",
    "    #         text = re.sub(n, \"\", text)\n",
    "    #\n",
    "    #     if stop:\n",
    "    #         print(\"skipping\")\n",
    "    #         investigate += [k]\n",
    "    #         continue\n",
    "    #\n",
    "    #     # now find the short names\n",
    "    #     for n, sn in short_names.items():\n",
    "    #         if bool(re.search(sn, text)):\n",
    "    #             short_name_dict[n] = np.union1d(short_name_dict[n], sn)\n",
    "    #     # replace the long names\n",
    "    #\n",
    "    #\n",
    "    # len([n for n, v in short_name_dict.items() if len(v) == 0])\n",
    "    #\n",
    "    #\n",
    "\n",
    "    # get the articles that have no names in text\n",
    "    # no_names_in_text = [f for f in client[\"news_articles\"][\"articles\"].find({'names_in_text': {\"$exists\": False}})]\n",
    "    # # save them local - must convery _id\n",
    "    # tmp = []\n",
    "    # for n in no_names_in_text:\n",
    "    #     n[\"_id\"] = str(\"_id\")\n",
    "    #     tmp.append(n)\n",
    "    # with open(get_data_path(\"articles_with_no_names_in_text.json\"), \"w\") as f:\n",
    "    #     json.dump(tmp, f)\n",
    "    # # drop those articles\n",
    "    # client[\"news_articles\"][\"articles\"].delete_many({'names_in_text': {\"$exists\": False}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Version of article_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ---\n",
    "# search for articles containing pair\n",
    "# ---\n",
    "\n",
    "# TODO: refactor this - search for parents first\n",
    "\n",
    "# find all the articles that contain parent nane\n",
    "pname = vc[\"Parent Name\"].unique()\n",
    "pdict = {}\n",
    "for i, pn in enumerate(pname):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"{i}/{len(pname)}\")\n",
    "    for k, v in articles.items():\n",
    "        if pn in v[\"names_in_text\"]:\n",
    "            if pn in pdict:\n",
    "                pdict[pn] += [k]\n",
    "            else:\n",
    "                pdict[pn] = [k]\n",
    "\n",
    "print(f\"number of Parent Company's found in articles: {len(pdict)}\")\n",
    "\n",
    "# store the articles containing the company pairs in a dict\n",
    "cpairs = {}\n",
    "for i, _ in enumerate(pdict.items()):\n",
    "    pn, pv = _\n",
    "    # for each parent get the Suppliers\n",
    "    # TODO: loosen this to get all company (i.e. include Customers)\n",
    "    cnames = vc.loc[(vc[\"Parent Name\"] == pn) & (vc[\"Relationship\"] == \"Supplier\"),\n",
    "                    \"Company Name\"].unique()\n",
    "    # for each company, check the articles the parent name is mentioned in\n",
    "    for cn in cnames:\n",
    "        # company is a supplier and customer to itself - skip\n",
    "        if pn == cn:\n",
    "            print(f\"skipping {pn}|{cn}\")\n",
    "            continue\n",
    "\n",
    "        pair = f\"{pn}|{cn}\"\n",
    "        for k in pv:\n",
    "            # if company is also in article store that\n",
    "            if cn in articles[k][\"names_in_text\"]:\n",
    "                if pair in cpairs:\n",
    "                    cpairs[pair] += [k]\n",
    "                else:\n",
    "                    cpairs[pair] = [k]\n",
    "\n",
    "# find the pair that is mentioned the most\n",
    "cpair_count = [(k, len(v)) for k, v in cpairs.items()]\n",
    "cpair_count = pd.DataFrame(cpair_count, columns=[\"pair\", \"count\"])\n",
    "cpair_count.sort_values(\"count\", ascending=False, inplace=True)\n",
    "\n",
    "# ----\n",
    "# find all the articles with supply|supplies|supplier in maintext\n",
    "# ----\n",
    "\n",
    "supply_articles = []\n",
    "for k, v in articles.items():\n",
    "    if re.search(\"supply|supplier|supplies\", v[\"maintext\"]):\n",
    "        supply_articles.append(k)\n",
    "\n",
    "print(f\"there are: {len(supply_articles)} that mention {'supply|supplier|supplies'}\")\n",
    "\n",
    "# ----\n",
    "# title count\n",
    "# ----\n",
    "\n",
    "all_titles = {\"titles\": [a['title'] for a in articles.values()]}\n",
    "all_titles = pd.DataFrame(all_titles)\n",
    "all_titles[\"count\"] = 1\n",
    "\n",
    "title_count = pd.pivot_table(all_titles, index=\"titles\", values=\"count\", aggfunc=\"count\")\n",
    "\n",
    "title_count.sort_values(\"count\", ascending=False, inplace=True)\n",
    "\n",
    "print(f\"there are: {len(title_count)} unique titles, {len(title_count.loc[title_count['count']> 1])} occur more than once\")\n",
    "\n",
    "# ----\n",
    "# for each company in value chain data - count of the number of articles in\n",
    "# ----\n",
    "\n",
    "article_count = {}\n",
    "# NOTE: maybe reverse this operation and just cycle through articles\n",
    "# - rather than names and articles ~ O(m*n)\n",
    "for n in all_names:\n",
    "    for k, v in articles.items():\n",
    "        if n in v[\"names_in_text\"]:\n",
    "            if n in article_count:\n",
    "                article_count[n] += 1\n",
    "            else:\n",
    "                article_count[n] = 1\n",
    "\n",
    "print(f\"there were {len(article_count)} companies found in articles\")\n",
    "\n",
    "c_count = pd.DataFrame([(k, v) for k, v in article_count.items()],\n",
    "                        columns=[\"name\", \"in_articles\"])\n",
    "c_count.sort_values(\"in_articles\", ascending=False, inplace=True)\n",
    "\n",
    "# ---\n",
    "# get name suffixes - in an attempt to make a 'short' name that often gets referenced in article\n",
    "# ---\n",
    "\n",
    "# TODO: short_name_map needs to be reviewed!, preferable to use some NLP package (spacy?)\n",
    "# This is pretty hard coded list of company name 'suffixes'\n",
    "# - some of these were taken by counting suffixes occrances, removing those and repeating\n",
    "# - others were just a gues\n",
    "suffixes = ['Inc', 'Corp', 'Ltd', 'Co', 'PLC', 'SA', 'AG', 'LLC', 'NV', 'SE',\n",
    "            'ASA', 'Bhd', 'SpA', 'Association', 'Aerospace', 'AB', 'Oyj'] + \\\n",
    "            ['Co', 'Holdings', 'Group', 'Technologies', 'International',\n",
    "            'Systems', 'Energy', 'Communications', 'Airlines', 'Motor',\n",
    "            'Technology', 'Oil', 'Motors', 'Corp', 'Industries', 'Steel',\n",
    "            'Holding', 'Airways', 'Aviation', 'Automotive', 'Networks',\n",
    "            'Electronics', 'Digital', 'BP', 'Electric', 'Aircraft',\n",
    "            'US', 'Mobile', 'Software', 'Broadcom', 'Brands',\n",
    "            'Service', 'Semiconductor', 'Petroleum'] + \\\n",
    "            ['Platforms', 'Precision', 'Industry', 'AeroSystems', 'Media', 'Petrochemical']\n",
    "\n",
    "\n",
    "\n",
    "#  'International Business Machines Corp' -> 'IBM'\n",
    "# 'News Corp' -> 'News Corp'\n",
    "# NOTE: if it starts with air it needs two words\n",
    "\n",
    "all_names = np.unique(np.concatenate([v[\"names_in_text\"] for k, v in articles.items()]))\n",
    "\n",
    "short_name = pd.DataFrame([(n, remove_suffix(n, suffixes)) for n in all_names],\n",
    "                            columns=[\"name\", \"short\"])\n",
    "# look at longer names\n",
    "short_name[\"len\"] = [len(n) for n in short_name[\"short\"]]\n",
    "short_name.sort_values(\"len\", ascending=False, inplace=True)\n",
    "\n",
    "# making a mapping dictionary\n",
    "short_name_map = {i[0]: i[1] for i in zip(short_name[\"name\"], short_name[\"short\"])}\n",
    "\n",
    "# HARDCODED!\n",
    "short_name_map['International Business Machines Corp'] = \"IBM\"\n",
    "short_name_map['News Corp'] = 'News Corp'\n",
    "short_name_map[\"Amazon.com Inc\"] = \"Amazon\"\n",
    "short_name_map[\"General Electric Co\"] = \"GE\"\n",
    "short_name_map[\"Lockheed Martin Corp\"] = \"Lockheed\"\n",
    "\n",
    "c_count[\"short_name\"] = c_count[\"name\"].map(short_name_map)\n",
    "\n",
    "# TODO: should check content to determine the which (short names?) are duplicated\n",
    "\n",
    "# -----\n",
    "# replace long names with short ones\n",
    "# -----\n",
    "\n",
    "# replace the long names with short names\n",
    "# - this is done because the short names are often used more\n",
    "keys = list(articles.keys())\n",
    "for k in keys:\n",
    "    a = articles[k]\n",
    "    for n in a['names_in_text']:\n",
    "        a[\"maintext\"] = re.sub(n, short_name_map[n], a[\"maintext\"])\n",
    "        # HARDCODE: replace ’ with ' - just a guess to try to deal with them\n",
    "        a[\"maintext\"] = re.sub(\"’\", \"'\", a[\"maintext\"])\n",
    "\n",
    "    a[\"short_names_in_text\"] = [short_name_map[n] for n in a[\"names_in_text\"]]\n",
    "    articles[k] = a\n",
    "\n",
    "# TODO: remove \"F2 percent\" ? LMT;-PCTCHNG:2} ?\n",
    "\n",
    "# ----\n",
    "# spacy\n",
    "# ----\n",
    "\n",
    "# TODO: perhaps want to get rid of sentences that start with \\nFILE PHOTO\n",
    "# - and end with \\nFILE PHOTO\n",
    "\n",
    "\n",
    "# https://spacy.io/usage/spacy-101#annotations\n",
    "\n",
    "# requires:$ python -m spacy download en_core_web_md\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# https://spacy.io/usage/rule-based-matching#entityruler\n",
    "ruler = nlp.add_pipe('entity_ruler')\n",
    "\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": v} for v in short_name_map.values()]\n",
    "ruler.add_patterns(patterns)\n",
    "# print(json.dumps(patterns, indent=4))\n",
    "\n",
    "# for articles that mention supply (or similar)\n",
    "# - found the company pairs\n",
    "\n",
    "# store results in a list\n",
    "out = []\n",
    "investigate = []\n",
    "# increment over each of the articles that reference supply\n",
    "for ii, k in enumerate(supply_articles):\n",
    "    if ii % 100 == 0:\n",
    "        print(f\"{ii} / {len(supply_articles)}\")\n",
    "    cps = [cp for cp, v in cpairs.items() if k in v]\n",
    "    text = articles[k][\"maintext\"]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sent_list = list(doc.sents)\n",
    "    sent_start = np.array([sent.start_char for sent in doc.sents])\n",
    "    sent_end = np.array([sent.end_char for sent in doc.sents])\n",
    "    # for each pair, get the short name\n",
    "    for cp in cps:\n",
    "        # long names\n",
    "        ln = cp.split(\"|\")\n",
    "        sn = [short_name_map[l] for l in ln]\n",
    "\n",
    "        # find the locations where entity 'a' is mentioned\n",
    "        # a = [(m.start(), m.end()) for m in re.finditer(sn[0], articles[k][\"maintext\"])]\n",
    "\n",
    "        try:\n",
    "            # using the entity recognition\n",
    "            # a = [(ent.start_char, ent.end_char) for ent in doc.ents if ent.text == sn[0]]\n",
    "            # TODO: review if can get deal with apostrophes on entity names through spacy\n",
    "            #  i.e. identify Lockheed Martin instead Lockheed Martin's as an entity\n",
    "            # NOTE: this will sometimes not even work: got 'Lockheed' 'Martin' as to separate entities\n",
    "            a = [(ent.start_char, ent.end_char) for ent in doc.ents if re.search(f\"^{sn[0]}\", ent.text)]\n",
    "            a = np.array(a)\n",
    "\n",
    "            # find the locations where entity 'b' is mentioned\n",
    "            # b = [(m.start(), m.end())  for m in re.finditer(sn[1], articles[k][\"maintext\"])]\n",
    "\n",
    "            # using the entity recognition\n",
    "\n",
    "            # b = [(ent.start_char, ent.end_char) for ent in doc.ents if ent.text == sn[1]]\n",
    "            b = [(ent.start_char, ent.end_char) for ent in doc.ents if re.search(f\"^{sn[1]}\", ent.text)]\n",
    "            b = np.array(b)\n",
    "\n",
    "            # find points in the text to connect the two, via sentence\n",
    "            start_pts, names = get_start_end(a=a[:,0], b=b[:,0], aname=sn[0], bname=sn[1])\n",
    "            end_pts, names = get_start_end(a[:,1], b[:,1], aname=sn[0], bname=sn[1])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            investigate.append([k, sn])\n",
    "\n",
    "        assert len(start_pts) == len(end_pts), \\\n",
    "            \"starting points and end points not to the, expect them to be\"\n",
    "\n",
    "        # get the full sentence\n",
    "        for i, sp in enumerate(start_pts):\n",
    "            nme = names[i]\n",
    "            ep = end_pts[i]\n",
    "            # find the start sentence location\n",
    "            # - by taking the start of the first entity, identify the sentences\n",
    "            # - where that is before the end and take the maximum\n",
    "            # TODO: this should be double checked/validated\n",
    "            sloc = np.argmax(sp[0] < sent_end)\n",
    "            # end sentence location\n",
    "            eloc = np.argmax(sp[1] < sent_end)\n",
    "\n",
    "            # s = sent_list[0]\n",
    "            # s.char_span(start_idx=0, end_idx=200)\n",
    "\n",
    "            # TODO: should there be a limit in the distance / number\n",
    "            #  of sentences inbetween?\n",
    "            # for now require difference to be less than equal to two\n",
    "            # NOTE: some sentence could just be \\n\n",
    "            if eloc - sloc <= 3:\n",
    "                # 'left' sentence starts\n",
    "                left_sent_start = sent_list[sloc].start_char\n",
    "                # take to where the first entity start\n",
    "                left = text[left_sent_start: sp[0]]\n",
    "                # middle is from end of first entity to start of next\n",
    "                middle = text[ep[0]:sp[1]]\n",
    "                # right is from end of second entity to the end of that sentence\n",
    "                right = text[ep[1]: sent_list[eloc].end_char]\n",
    "\n",
    "                # full_sentence = left + nme[0] + middle + nme[1] + right\n",
    "\n",
    "                # store results in a list\n",
    "                out.append([nme[0], nme[1], left, middle, right, k])\n",
    "\n",
    "df = pd.DataFrame(out, columns=[\"entity1\", \"entity2\", \"left\", \"middle\", \"right\", \"article\"])\n",
    "\n",
    "df.to_csv(get_data_path(\"example_inputs.tsv\"), sep=\"\\t\", index=False)\n",
    "\n",
    "# doc = nlp(\"A complex-example,!\")\n",
    "# print([token.text for token in doc])\n",
    "#\n",
    "# a = articles[keys[1]]\n",
    "# t0 = time.perf_counter()\n",
    "# doc = nlp(a[\"maintext\"])\n",
    "# t1 = time.perf_counter()\n",
    "# # will take about ~ 11min to run fo 5500 articles\n",
    "#\n",
    "# print(a[\"short_names_in_text\"])\n",
    "# print(json.dumps([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"ORG\"], indent=4))\n",
    "# # for token in doc:\n",
    "# #     print(token.text)\n",
    "# print(a[\"names_in_text\"])\n",
    "#\n",
    "# for ent in doc.ents:\n",
    "#     if ent.label_ == \"ORG\":\n",
    "#         print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "#     # if ent.label_ == \"GPE\":\n",
    "    # if re.search(\"Alphabet\", ent.text):\n",
    "    #     print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "# from spacy import displacy\n",
    "# displacy.serve(doc, style=\"dep\")\n",
    "\n",
    "\n",
    "# # ---\n",
    "# # replace names with dash separated - in an attempt to deal with short names later\n",
    "# # ---\n",
    "#\n",
    "# # TODO: drop this - just use spacy?\n",
    "#\n",
    "# all_names = np.unique(np.concatenate([v[\"names_in_text\"]\n",
    "#                                       for k, v in articles.items()]))\n",
    "# dash_names = {n: re.sub(\" \", \"-\", n) for n in all_names}\n",
    "#\n",
    "# # for each article replace the name with the dashed name\n",
    "# # - the idea is this will help with tokenizing\n",
    "#\n",
    "#\n",
    "# for k in keys:\n",
    "#     a = articles[k]\n",
    "#     anames = np.array(a[\"names_in_text\"])\n",
    "#     dnames = np.array([dash_names[an] for an in anames])\n",
    "#     # replace the longest matching first - to deal with\n",
    "#     name_len = [len(an) for an in anames]\n",
    "#     name_ord = np.argsort(name_len)[::-1]\n",
    "#\n",
    "#     maintext = a[\"maintext\"]\n",
    "#     name_in_article = []\n",
    "#     for an in anames[name_ord]:\n",
    "#         if re.search(an, maintext):\n",
    "#\n",
    "#             name_in_article.append(an)\n",
    "#             maintext = re.sub(an, dash_names[an], maintext)\n",
    "#         else:\n",
    "#             # print(f\"could not find: {an} in {k}\")\n",
    "#             pass\n",
    "#\n",
    "#     a[\"names_in_text\"] = name_in_article\n",
    "#\n",
    "# # keep only those articles with two or more names_in_text\n",
    "# articles = {k: v for k, v in articles.items() if len(v[\"names_in_text\"]) > 1}\n",
    "\n",
    "\n",
    "# tmp = [a for a in articles.values() if 'News Corp' in a[\"names_in_text\"]]\n",
    "#\n",
    "#\n",
    "# all_sufs = pd.DataFrame([ (n, n.split(\" \")[-1])for n in all_names],\n",
    "#                         columns=[\"name\", \"suf\"])\n",
    "# suf_count = pd.pivot_table(all_sufs,\n",
    "#                            index=\"suf\",\n",
    "#                            values=\"name\",\n",
    "#                            aggfunc=\"count\").reset_index()\n",
    "#\n",
    "# suf_count.sort_values(\"name\", ascending=False, inplace=True)\n",
    "#\n",
    "# # drop the more common suffixes - in an attempt to make short names\n",
    "# suf_vals = suf_count.loc[suf_count[\"name\"] >= 2, \"suf\"].values\n",
    "# # regular expression for matching suffixes\n",
    "# suf_regex = \"\".join([f\" {sv}$|\" for sv in suf_vals])\n",
    "#\n",
    "# # get the second suffix\n",
    "# suf2 = pd.DataFrame([(n, re.sub(suf_regex, \"\", n).split(\" \")[-1])\n",
    "#                      for n in all_names if bool(re.search(suf_regex, n))],\n",
    "#                     columns=[\"name\", \"suf\"])\n",
    "#\n",
    "# suf_count2 = pd.pivot_table(suf2,\n",
    "#                            index=\"suf\",\n",
    "#                            values=\"name\",\n",
    "#                            aggfunc=\"count\").reset_index()\n",
    "# suf_count2.sort_values(\"name\", ascending=False, inplace=True)\n",
    "#\n",
    "# # do another suffix count\n",
    "# suf_vals2 = suf_count2.loc[suf_count2[\"name\"] >= 2, \"suf\"].values\n",
    "# suf_regex2 = \"\".join([f\" {sv}$|\" for sv in suf_vals2])\n",
    "#\n",
    "# short_names = {n: re.sub(suf_regex2, \"\", re.sub(suf_regex, \"\", n))\n",
    "#                for n in all_names}\n",
    "#\n",
    "# tmp = [a for a in articles.values() if 'Meta Platforms Inc' in a[\"names_in_text\"]]\n",
    "# tmp = [a for a in articles.values() if 'Electronic Arts Inc' in a[\"names_in_text\"]]\n",
    "\n",
    "\n",
    "# HARDCODED: ['Exxon Mobil Corp', 'Mobil Corp'] were erronously mathced?\n",
    "# drop_exxon_mobil = []\n",
    "# for k in articles.keys():\n",
    "#     if np.in1d(articles[k][\"names_in_text\"], ['Exxon Mobil Corp', 'Mobil Corp']).all():\n",
    "#         drop_exxon_mobil.append(k)\n",
    "#     # just remove Mobil Corp - this isn't robust\n",
    "#     if np.in1d(['Mobil Corp'], articles[k][\"names_in_text\"]).any():\n",
    "#         articles[k][\"names_in_text\"] = [n for n in articles[k][\"names_in_text\"] if n != \"Mobil Corp\"]\n",
    "#\n",
    "# for k in drop_exxon_mobil:\n",
    "#     articles.pop(k)\n",
    "\n",
    "# # ---\n",
    "# # rules for getting short names\n",
    "# # ---\n",
    "#\n",
    "# all_names = np.unique(np.concatenate([v[\"names_in_text\"]\n",
    "#                                       for k, v in articles.items()]))\n",
    "#\n",
    "# tmp = find_all_articles_with_name(articles, 'Ryanair Holdings PLC')\n",
    "#\n",
    "# # Rule 1: first name\n",
    "# # short_names = [n.split(\" \")[0] for n in all_names]\n",
    "#\n",
    "# # keep track of articles where did not find all names\n",
    "# not_found_all_names = []\n",
    "#\n",
    "# #\n",
    "# nlp = English()\n",
    "# tokenizer = nlp.tokenizer\n",
    "#\n",
    "# # record all the short names\n",
    "# short_name_dict = {n: np.array([]) for n in all_names}\n",
    "#\n",
    "# investigate = []\n",
    "# for i, _ in enumerate(articles.items()):\n",
    "#\n",
    "#     stop = False\n",
    "#     k, v = _\n",
    "#     names = v[\"names_in_text\"]\n",
    "#     # Rule 1: first name\n",
    "#     short_names = {n: n.split(\" \")[0] for n in names}\n",
    "#\n",
    "#     # tokenize the data\n",
    "#     # - is there a better, more correct way of doing this?\n",
    "#     text = v['maintext']\n",
    "#\n",
    "#     # confirm the long names are in text\n",
    "#     # - then remove it\n",
    "#     for n in names:\n",
    "#\n",
    "#         if not bool(re.search(n, text)):\n",
    "#             print(f\"{n} not found in text, expected to!\")\n",
    "#             print(f\"names list: {names}\")\n",
    "#             stop = True\n",
    "#             break\n",
    "#\n",
    "#         # remove the long names\n",
    "#         text = re.sub(n, \"\", text)\n",
    "#\n",
    "#     if stop:\n",
    "#         print(\"skipping\")\n",
    "#         investigate += [k]\n",
    "#         continue\n",
    "#\n",
    "#     # now find the short names\n",
    "#     for n, sn in short_names.items():\n",
    "#         if bool(re.search(sn, text)):\n",
    "#             short_name_dict[n] = np.union1d(short_name_dict[n], sn)\n",
    "#     # replace the long names\n",
    "#\n",
    "#\n",
    "# len([n for n, v in short_name_dict.items() if len(v) == 0])\n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e205b358c613ea824e49ff2da80714f6475c17ffffa6b4104d0c9f7fa9092ac7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('comp0084-cw1': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
