{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation extraction using distant supervision: experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Bill MacCartney and Christopher Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "1. [Set-up](#Set-up)\n",
    "1. [Building a classifier](#Building-a-classifier)\n",
    "  1. [Featurizers](#Featurizers)\n",
    "  1. [Experiments](#Experiments)\n",
    "1. [Analysis](#Analysis)\n",
    "  1. [Examining the trained models](#Examining-the-trained-models)\n",
    "  1. [Discovering new relation instances](#Discovering-new-relation-instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "OK, it's time to get (halfway) serious. Let's apply real machine learning to train a classifier on the training data, and see how it performs on the test data. We'll begin with one of the simplest machine learning setups: a bag-of-words feature representation, and a linear model trained using logistic regression.\n",
    "\n",
    "Just like we did in the unit on [supervised sentiment analysis](https://github.com/cgpotts/cs224u/blob/master/sst_02_hand_built_features.ipynb), we'll leverage the `sklearn` library, and we'll introduce functions for featurizing instances, training models, making predictions, and evaluating results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "See [the first notebook in this unit](rel_ext_01_task.ipynb#Set-up) for set-up instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import rel_ext\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the random seeds for reproducibility. Only the\n",
    "# system seed is relevant for this notebook.\n",
    "\n",
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # python package (nlp) location - two levels up from this file\n",
    "    src_path = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "    # add package to sys.path if it's not already there\n",
    "    if src_path not in sys.path:\n",
    "        sys.path.extend([src_path])\n",
    "except NameError:\n",
    "    print('issue with adding to path, probably due to __file__ not being defined')\n",
    "    src_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/timmaecker/Google Drive/Career and Skills/Learning/MSc Machine Learning - UCL/2 COMP0087 Statistical Natural Language Processing/Coursework/nlpproject/nlp/data'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_ext_data_home = os.path.join(src_path,'data')\n",
    "rel_ext_data_home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following steps, we build up the dataset we'll use for experiments; it unites a corpus and a knowledge base in the way we described in [the previous notebook](rel_ext_01_task.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 86,298 examples\n"
     ]
    }
   ],
   "source": [
    "corpus = rel_ext.Corpus(os.path.join(rel_ext_data_home, 'example_inputs_long_names_pos_and_neg.tsv.gz'))\n",
    "\n",
    "print('Read {0:,} examples'.format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 21,458 KB triples\n"
     ]
    }
   ],
   "source": [
    "kb = rel_ext.KB(os.path.join(rel_ext_data_home, 'example_kb.tsv.gz'))\n",
    "\n",
    "print('Read {0:,} KB triples'.format(len(kb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rel_ext.Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code splits up our data in a way that supports experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiny': Corpus with 4,151 examples; KB with 244 triples,\n",
       " 'train': Corpus with 61,481 examples; KB with 15,632 triples,\n",
       " 'dev': Corpus with 20,666 examples; KB with 5,582 triples,\n",
       " 'all': Corpus with 86,298 examples; KB with 21,458 triples}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = dataset.build_splits()\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurizers\n",
    "\n",
    "Featurizers are functions which define the feature representation for our model. The primary input to a featurizer will be the `KBTriple` for which we are generating features. But since our features will be derived from corpus examples containing the entities of the `KBTriple`, we must also pass in a reference to a `Corpus`. And in order to make it easy to combine different featurizers, we'll also pass in a feature counter to hold the results.\n",
    "\n",
    "Here's an implementation for a very simple bag-of-words featurizer. It finds all the corpus examples containing the two entities in the `KBTriple`, breaks the phrase appearing between the two entity mentions into words, and counts the words. Note that it makes no distinction between \"forward\" and \"reverse\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.middle.split(' '):\n",
    "            feature_counter[word] += 1\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how this featurizer works on a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KBTriple(rel='supplier of', sbj='Iri Group Holdings Inc', obj='DS Smith PLC')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbt = kb.kb_triples[2]\n",
    "\n",
    "kbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iri Group Holdings Inc'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbt.sbj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DS Smith PLC'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbt.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_examples_for_entities(kbt.sbj, kbt.obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sprint Corp Brightstar Corp\n",
      " and SoftBank, is likely to accelerate under new chief executive Marcelo Claure, plucked from handset reseller \n"
     ]
    }
   ],
   "source": [
    "#Find for which triplets we have examples\n",
    "for kbt in kb.kb_triples:\n",
    "    if len(corpus.get_examples_for_entities(kbt.sbj, kbt.obj))>0:\n",
    "        print(kbt.sbj, kbt.obj)\n",
    "        print(corpus.get_examples_for_entities(kbt.sbj, kbt.obj)[0].middle)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and SoftBank, is likely to accelerate under new chief executive Marcelo Claure, plucked from handset reseller '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_examples_for_entities(kbt.sbj, kbt.obj)[0].middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example(entity_1='Sprint Corp', entity_2='Brightstar Corp', left=\"Sprint's aggressive strategy on new handsets and price cuts, with the unveiling last week of a smartphone developed by Sharp exclusive for \", mention_1='Sprint Corp', middle=' and SoftBank, is likely to accelerate under new chief executive Marcelo Claure, plucked from handset reseller ', mention_2='Brightstar Corp', right=' which SoftBank acquired last year.', left_POS=\"Sprint's aggressive strategy on new handsets and price cuts, with the unveiling last week of a smartphone developed by Sharp exclusive for \", mention_1_POS='Sprint Corp', middle_POS=' and SoftBank, is likely to accelerate under new chief executive Marcelo Claure, plucked from handset reseller ', mention_2_POS='Brightstar Corp', right_POS=' which SoftBank acquired last year.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_examples_for_entities(kbt.sbj, kbt.obj)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'': 8,\n",
       "         'and': 4,\n",
       "         'SoftBank,': 2,\n",
       "         'is': 2,\n",
       "         'likely': 2,\n",
       "         'to': 2,\n",
       "         'accelerate': 2,\n",
       "         'under': 2,\n",
       "         'new': 2,\n",
       "         'chief': 2,\n",
       "         'executive': 2,\n",
       "         'Marcelo': 2,\n",
       "         'Claure,': 2,\n",
       "         'plucked': 2,\n",
       "         'from': 2,\n",
       "         'handset': 2,\n",
       "         'reseller': 2,\n",
       "         'which': 2,\n",
       "         'SoftBank': 2,\n",
       "         'acquired': 2,\n",
       "         'last': 2,\n",
       "         'year.SoftBank': 2,\n",
       "         'CEO': 2,\n",
       "         'founder': 2,\n",
       "         'Masayoshi': 2,\n",
       "         'Son': 2,\n",
       "         'has': 2,\n",
       "         'vowed': 2,\n",
       "         'bold': 2,\n",
       "         'moves': 2,\n",
       "         'by': 2})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_bag_of_words_featurizer(kbt, corpus, Counter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with adding new kinds of features just by implementing additional featurizers, following `simple_bag_of_words_featurizer` as an example.\n",
    "\n",
    "Now, in order to apply machine learning algorithms such as those provided by `sklearn`, we need a way to convert datasets of `KBTriple`s into feature matrices. The following steps achieve that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbts_by_rel, labels_by_rel = dataset.build_dataset()\n",
    "\n",
    "featurized = dataset.featurize(kbts_by_rel, featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "Now we need some functions to train models, make predictions, and evaluate the results. We'll start with `train_models()`. This function takes as arguments a dictionary of data splits, a list of featurizers, the name of the split on which to train (by default, 'train'), and a model factory, which is a function which initializes an `sklearn` classifier (by default, a logistic regression classifier). It returns a dictionary holding the featurizers, the vectorizer that was used to generate the training matrix, and a dictionary holding the trained models, one per relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timmaecker/opt/miniconda3/envs/nlpwcw1_rec/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/timmaecker/opt/miniconda3/envs/nlpwcw1_rec/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_result = rel_ext.train_models(\n",
    "    splits,\n",
    "    featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes `predict()`. This function takes as arguments a dictionary of data splits, the outputs of `train_models()`, and the name of the split for which to make predictions. It returns two parallel dictionaries: one holding the predictions (grouped by relation), the other holding the true labels (again, grouped by relation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, true_labels = rel_ext.predict(\n",
    "    splits, train_result, split_name='dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `evaluate_predictions()`. This function takes as arguments the parallel dictionaries of predictions and true labels produced by `predict()`. It prints summary statistics for each relation, including precision, recall, and F<sub>0.5</sub>-score, and it returns the macro-averaged F<sub>0.5</sub>-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "customer of               0.902      0.986      0.917       2557       2869\n",
      "supplier of               0.922      0.977      0.932       3025       3337\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.912      0.982      0.925       5582       6206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9248700840206208"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_ext.evaluate_predictions(predictions, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we introduce `rel_ext.experiment()`, which basically chains together `rel_ext.train_models()`, `rel_ext.predict()`, and `rel_ext.evaluate_predictions()`. For convenience, this function returns the output of `rel_ext.train_models()` as its result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `rel_ext.experiment()` in its default configuration will give us a baseline result for machine-learned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timmaecker/opt/miniconda3/envs/nlpwcw1_rec/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/timmaecker/opt/miniconda3/envs/nlpwcw1_rec/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "customer of               0.902      0.986      0.917       2557       2869\n",
      "supplier of               0.922      0.977      0.932       3025       3337\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.912      0.982      0.925       5582       6206\n"
     ]
    }
   ],
   "source": [
    "_ = rel_ext.experiment(\n",
    "    splits,\n",
    "    featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering how vanilla our model is, these results are quite surprisingly good! We see huge gains for every relation over our `top_k_middles_classifier` from [the previous notebook](rel_ext_01_task.ipynb#A-simple-baseline-model). This strong performance is a powerful testament to the effectiveness of even the simplest forms of machine learning.\n",
    "\n",
    "But there is still much more we can do. To make further gains, we must not treat the model as a black box. We must open it up and get visibility into what it has learned, and more importantly, where it still falls down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the trained models\n",
    "\n",
    "One important way to gain understanding of our trained model is to inspect the model weights. What features are strong positive indicators for each relation, and what features are strong negative indicators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timmaecker/opt/miniconda3/envs/nlpwcw1_rec/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = train_result['vectorizer']\n",
    "\n",
    "if vectorizer is None:\n",
    "    print(\"Model weights can be examined only if the featurizers \"\n",
    "            \"are based in dicts (i.e., if `vectorize=True`).\")\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation customer of:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rel, model in train_result['models'].items():\n",
    "    print('Highest and lowest feature weights for relation {}:\\n'.format(rel))\n",
    "    try:\n",
    "        coefs = model.coef_.toarray()\n",
    "    except AttributeError:\n",
    "        coefs = model.coef_\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46556"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coefs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation customer of:\n",
      "\n",
      "     0.273 's\n",
      "     0.231 chips\n",
      "     0.203 week\n",
      "     0.202 \"\n",
      "     0.199 their\n",
      "     0.194 chip\n",
      "     0.191 have\n",
      "     0.180 some\n",
      "     0.163 China\n",
      "     0.161 sales\n",
      "     0.161 plant\n",
      "     0.160 05-Feb-2020\n",
      "     0.158 fall\n",
      "     0.157 AMC\n",
      "     0.155 New\n",
      "     0.154 this\n",
      "     0.149 mobile\n",
      "     0.142 was\n",
      "     0.135 General\n",
      "     0.131 supplier\n",
      "     ..... .....\n",
      "    -0.163 group\n",
      "    -0.163 than\n",
      "    -0.164 helped\n",
      "    -0.166 expectations\n",
      "    -0.166 lost\n",
      "    -0.168 Chief\n",
      "    -0.169 operating\n",
      "    -0.170 carriers\n",
      "    -0.175 Release\n",
      "    -0.179 Inc,\n",
      "    -0.184 airline\n",
      "    -0.189 Holdings\n",
      "    -0.195 Qorvo\n",
      "    -0.199 read\n",
      "    -0.207 :\n",
      "    -0.215 Ltd\n",
      "    -0.230 Inc\n",
      "    -0.237 Corp\n",
      "    -0.241 down\n",
      "    -0.382 firms\n",
      "\n",
      "Highest and lowest feature weights for relation supplier of:\n",
      "\n",
      "     0.442 launch\n",
      "     0.405 's\n",
      "     0.392 chip\n",
      "     0.391 chips\n",
      "     0.386 products\n",
      "     0.384 this\n",
      "     0.375 2019\n",
      "     0.373 2019.\n",
      "     0.371 their\n",
      "     0.367 plant\n",
      "     0.362 Microsoft\n",
      "     0.360 05-Feb-2020\n",
      "     0.357 our\n",
      "     0.354 some\n",
      "     0.348 year\n",
      "     0.343 sales\n",
      "     0.343 3\n",
      "     0.340 week\n",
      "     0.337 results\n",
      "     0.333 industry\n",
      "     ..... .....\n",
      "    -0.408 Inc's\n",
      "    -0.408 airline\n",
      "    -0.411 Intel\n",
      "    -0.416 among\n",
      "    -0.419 Indian\n",
      "    -0.426 Holdings\n",
      "    -0.433 Apple\n",
      "    -0.435 group\n",
      "    -0.437 recent\n",
      "    -0.439 than\n",
      "    -0.448 include\n",
      "    -0.452 Inc,\n",
      "    -0.474 other\n",
      "    -0.485 dropped\n",
      "    -0.497 contract\n",
      "    -0.528 read\n",
      "    -0.550 Nikkei\n",
      "    -0.558 carriers\n",
      "    -0.572 firms\n",
      "    -0.691 Corp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k=20\n",
    "for rel, model in train_result['models'].items():\n",
    "    print('Highest and lowest feature weights for relation {}:\\n'.format(rel))\n",
    "    try:\n",
    "        coefs = model.coef_.toarray()\n",
    "    except AttributeError:\n",
    "        coefs = model.coef_\n",
    "    sorted_weights = sorted([(wgt, idx) for idx, wgt in enumerate(coefs[0])], reverse=True)\n",
    "    for wgt, idx in sorted_weights[:k]:\n",
    "        print('{:10.3f} {}'.format(wgt, feature_names[idx]))\n",
    "    print('{:>10s} {}'.format('.....', '.....'))\n",
    "    for wgt, idx in sorted_weights[-k:]:\n",
    "        print('{:10.3f} {}'.format(wgt, feature_names[idx]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation customer of:\n",
      "\n",
      "     0.273 's\n",
      "     0.231 chips\n",
      "     0.203 week\n",
      "     ..... .....\n",
      "    -0.237 Corp\n",
      "    -0.241 down\n",
      "    -0.382 firms\n",
      "\n",
      "Highest and lowest feature weights for relation supplier of:\n",
      "\n",
      "     0.442 launch\n",
      "     0.405 's\n",
      "     0.392 chip\n",
      "     ..... .....\n",
      "    -0.558 carriers\n",
      "    -0.572 firms\n",
      "    -0.691 Corp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.examine_model_weights(train_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By and large, the high-weight features for each relation are pretty intuitive — they are words that are used to express the relation in question. (The counter-intuitive results merit a bit of investigation!)\n",
    "\n",
    "The low-weight features (that is, features with large negative weights) may be a bit harder to understand. In some cases, however, they can be interpreted as features which indicate some _other_ relation which is anti-correlated with the target relation. (As an example, \"directed\" is a negative indicator for the `author` relation.)\n",
    "\n",
    "__Optional exercise:__ Investigate one of the counter-intuitive high-weight features. Find the training examples which caused the feature to be included. Given the training data, does it make sense that this feature is a good predictor for the target relation?\n",
    "\n",
    "<!--\n",
    "- SPOILER: Using `penalty='l1'` results in somewhat less intuitive feature weights, and about the same performance.\n",
    "- SPOILER: Using `penalty='l1', C=0.1` results in much more intuitive feature weights, but much worse performance.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering new relation instances\n",
    "\n",
    "Another way to gain insight into our trained models is to use them to discover new relation instances that don't currently appear in the KB. In fact, this is the whole point of building a relation extraction system: to extend an existing KB (or build a new one) using knowledge extracted from natural language text at scale. Can the models we've trained do this effectively?\n",
    "\n",
    "Because the goal is to discover new relation instances which are *true* but *absent from the KB*, we can't evaluate this capability automatically. But we can generate candidate KB triples and manually evaluate them for correctness.\n",
    "\n",
    "To do this, we'll start from corpus examples containing pairs of entities which do not belong to any relation in the KB (earlier, we described these as \"negative examples\"). We'll then apply our trained models to each pair of entities, and sort the results by probability assigned by the model, in order to find the most likely new instances for each relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timmaecker/opt/miniconda3/envs/nlpwcw1_rec/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/Users/timmaecker/opt/miniconda3/envs/nlpwcw1_rec/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest probability examples for relation customer of:\n",
      "\n",
      "     1.000 KBTriple(rel='customer of', sbj='Amazon.com Inc', obj='Microsoft Corp')\n",
      "     1.000 KBTriple(rel='customer of', sbj='Microsoft Corp', obj='Amazon.com Inc')\n",
      "     1.000 KBTriple(rel='customer of', sbj='Tenaga Nasional Bhd', obj='Petronas Dagangan Bhd')\n",
      "     1.000 KBTriple(rel='customer of', sbj='Petronas Dagangan Bhd', obj='Tenaga Nasional Bhd')\n",
      "     1.000 KBTriple(rel='customer of', sbj='Walmart Inc', obj='Amazon.com Inc')\n",
      "     1.000 KBTriple(rel='customer of', sbj='Amazon.com Inc', obj='Walmart Inc')\n",
      "     0.999 KBTriple(rel='customer of', sbj='Dell Inc', obj='General Electric Co')\n",
      "     0.999 KBTriple(rel='customer of', sbj='General Electric Co', obj='Dell Inc')\n",
      "     0.998 KBTriple(rel='customer of', sbj='Nokia Oyj', obj='Shell PLC')\n",
      "     0.998 KBTriple(rel='customer of', sbj='Shell PLC', obj='Nokia Oyj')\n",
      "\n",
      "Highest probability examples for relation supplier of:\n",
      "\n",
      "     1.000 KBTriple(rel='supplier of', sbj='Maxis Bhd', obj='UMW Holdings Bhd')\n",
      "     1.000 KBTriple(rel='supplier of', sbj='UMW Holdings Bhd', obj='Maxis Bhd')\n",
      "     1.000 KBTriple(rel='supplier of', sbj='Tenaga Nasional Bhd', obj='Maxis Bhd')\n",
      "     1.000 KBTriple(rel='supplier of', sbj='Maxis Bhd', obj='Tenaga Nasional Bhd')\n",
      "     1.000 KBTriple(rel='supplier of', sbj='Petronas Dagangan Bhd', obj='UMW Holdings Bhd')\n",
      "     1.000 KBTriple(rel='supplier of', sbj='UMW Holdings Bhd', obj='Petronas Dagangan Bhd')\n",
      "     1.000 KBTriple(rel='supplier of', sbj='Tenaga Nasional Bhd', obj='Petronas Dagangan Bhd')\n",
      "     1.000 KBTriple(rel='supplier of', sbj='Petronas Dagangan Bhd', obj='Tenaga Nasional Bhd')\n",
      "     1.000 KBTriple(rel='supplier of', sbj='Petroliam Nasional Bhd', obj='Maxis Bhd')\n",
      "     1.000 KBTriple(rel='supplier of', sbj='Maxis Bhd', obj='Petroliam Nasional Bhd')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.find_new_relation_instances(\n",
    "    dataset,\n",
    "    featurizers=[simple_bag_of_words_featurizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually some good discoveries here! The predictions for the `author` relation seem especially good. Of course, there are also plenty of bad results, and a few that are downright comical. We may hope that as we improve our models and optimize performance in our automatic evaluations, the results we observe in this manual evaluation improve as well.\n",
    "\n",
    "__Optional exercise:__ Note that every time we predict that a given relation holds between entities `X` and `Y`, we also predict, with equal confidence, that it holds between `Y` and `X`. Why? How could we fix this?\n",
    "\n",
    "\\[ [top](#Relation-extraction-using-distant-supervision) \\]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4afa26c81afa363b1ab4bf815135f7b681d0cb95b1a3acd8faa5c509a97d1404"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('nlpwcw1_rec': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
